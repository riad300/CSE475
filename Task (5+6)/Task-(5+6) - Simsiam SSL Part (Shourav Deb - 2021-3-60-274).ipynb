{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task (4+5+6) - Simsiam 2nd SSL Part\n",
    "### Shourav Deb [2021-3-60-274]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 4.1) CELL 1 - Header / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:09:27.733561Z",
     "iopub.status.busy": "2025-12-05T08:09:27.733399Z",
     "iopub.status.idle": "2025-12-05T08:09:34.145639Z",
     "shell.execute_reply": "2025-12-05T08:09:34.144962Z",
     "shell.execute_reply.started": "2025-12-05T08:09:27.733544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG\n",
      "DATA_DIR: /kaggle/input/betel-leaf/Betel Leaf Dataset A Primary Dataset From Field And Controlled Environment/Betel Leaf Dataset\n",
      "RESOLUTION: 224\n",
      "PRETRAIN_EPOCHS: 100\n",
      "BATCH_SIZE: 64\n",
      "BACKBONE: resnet18\n",
      "OUT_DIR: /kaggle/working/simsiam_task4\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SimSiam - Config\n",
    "# =========================\n",
    "\n",
    "import os, random, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/betel-leaf/Betel Leaf Dataset A Primary Dataset From Field And Controlled Environment/Betel Leaf Dataset\"\n",
    "\n",
    "RESOLUTION = 224\n",
    "\n",
    "PRETRAIN_EPOCHS = 100\n",
    "\n",
    "LINEAR_EPOCHS = 50\n",
    "FINETUNE_EPOCHS = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BACKBONE = \"resnet18\"\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "OUT_DIR = \"/kaggle/working/simsiam_task4\"\n",
    "\n",
    "\n",
    "assert os.path.exists(DATA_DIR), f\"DATA_DIR not found: {DATA_DIR}\"\n",
    "assert PRETRAIN_EPOCHS >= 100, \"PRETRAIN_EPOCHS must be >= 100\"\n",
    "assert isinstance(RESOLUTION, int) and RESOLUTION >= 64, \"RESOLUTION must be integer >=64\"\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CONFIG\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"RESOLUTION:\", RESOLUTION)\n",
    "print(\"PRETRAIN_EPOCHS:\", PRETRAIN_EPOCHS)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "print(\"BACKBONE:\", BACKBONE)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2 - Imports & Basic Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports & Utilities\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
    "os.environ.setdefault(\"XLA_FLAGS\", \"--xla_gpu_cuda_data_dir=/usr/local/cuda  --xla_force_host_platform_device_count=1\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception as e:\n",
    "    sns = None\n",
    "    print(\"Warning: seaborn import failed — continuing without it:\", e)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn, optim\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    import torchvision\n",
    "    from torchvision import transforms, models\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    optim = None\n",
    "    DataLoader = None\n",
    "    Dataset = None\n",
    "    torchvision = None\n",
    "    transforms = None\n",
    "    models = None\n",
    "    print(\"Warning: PyTorch imports failed or CUDA unavailable:\", e)\n",
    "\n",
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except Exception as e:\n",
    "    print(\"Warning: scikit-learn import failed:\", e)\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception as e:\n",
    "    umap = None\n",
    "    print(\"Info: umap not available:\", e)\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.metrics import silhouette_score\n",
    "except Exception as e:\n",
    "    print(\"Warning importing TSNE / silhouette_score:\", e)\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    import pickle\n",
    "except Exception as e:\n",
    "    print(\"Warning: joblib/pickle import issue:\", e)\n",
    "\n",
    "def env_diagnostics(show_packages: Optional[list] = None):\n",
    "    \"\"\"Print device + common package versions to help debug environment mismatches.\"\"\"\n",
    "    print(\"Python:\", sys.version.splitlines()[0])\n",
    "    # PyTorch & CUDA\n",
    "    if torch is not None:\n",
    "        try:\n",
    "            print(\"PyTorch:\", torch.__version__)\n",
    "            print(\"CUDA available:\", torch.cuda.is_available())\n",
    "            if torch.cuda.is_available():\n",
    "                print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "                print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "                print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        except Exception as e:\n",
    "            print(\"PyTorch diagnostic error:\", e)\n",
    "    else:\n",
    "        print(\"PyTorch: not available\")\n",
    "\n",
    "    # seaborn / matplotlib\n",
    "    try:\n",
    "        import matplotlib\n",
    "        print(\"matplotlib:\", matplotlib.__version__)\n",
    "    except Exception:\n",
    "        print(\"matplotlib: not available\")\n",
    "\n",
    "    if sns is not None:\n",
    "        try:\n",
    "            print(\"seaborn:\", sns.__version__)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # scikit-learn\n",
    "    try:\n",
    "        import sklearn\n",
    "        print(\"scikit-learn:\", sklearn.__version__)\n",
    "    except Exception:\n",
    "        print(\"scikit-learn: not available\")\n",
    "\n",
    "    # umap\n",
    "    if umap is not None:\n",
    "        try:\n",
    "            print(\"umap-learn:\", umap.__version__)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        import importlib\n",
    "        if importlib.util.find_spec(\"tensorflow\") is not None:\n",
    "            import tensorflow as tf\n",
    "            print(\"TensorFlow:\", tf.__version__)\n",
    "           \n",
    "        else:\n",
    "            print(\"TensorFlow: not installed (or not found in this env)\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(\"TensorFlow import safe-check raised an exception (not fatal):\", e)\n",
    "\n",
    "\n",
    "if torch is not None:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "\n",
    "env_diagnostics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3 - Build file manifest (reads dataset structure & prints counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:11:50.291756Z",
     "iopub.status.busy": "2025-12-05T08:11:50.291094Z",
     "iopub.status.idle": "2025-12-05T08:11:50.334294Z",
     "shell.execute_reply": "2025-12-05T08:11:50.333551Z",
     "shell.execute_reply.started": "2025-12-05T08:11:50.291722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected top-level source folders: ['On Field', 'Controlled Environment']\n",
      "\n",
      "Source 'On Field' subfolders: ['Diseased Leaf', 'Healthy Leaf', 'Dried Leaf']\n",
      "\n",
      "Source 'Controlled Environment' subfolders: ['Diseased', 'Dried', 'Healthy']\n",
      "\n",
      "Total images found: 1800\n",
      "Per-subfolder counts:\n",
      "  Diseased Leaf: 289\n",
      "  Dried Leaf: 282\n",
      "  Healthy Leaf: 336\n",
      "  Diseased: 220\n",
      "  Dried: 340\n",
      "  Healthy: 333\n",
      "\n",
      "Manifest saved to /kaggle/working/simsiam_task4/manifest.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, os\n",
    "root = Path(\"/kaggle/input/betel-leaf/Betel Leaf Dataset A Primary Dataset From Field And Controlled Environment/Betel Leaf Dataset\")\n",
    "assert root.exists(), f\"Dataset root missing: {root}\"\n",
    "\n",
    "expected_classes = [\"Diseased\", \"Dried\", \"Healthy\"]\n",
    "sources = [p.name for p in root.iterdir() if p.is_dir()]\n",
    "print(\"Detected top-level source folders:\", sources)\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "found_map = {}\n",
    "\n",
    "def normalize(name: str):\n",
    "    \"\"\"Utility to normalize folder names for matching.\"\"\"\n",
    "    return name.lower().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "for src in sources:\n",
    "    src_dir = root / src\n",
    "    subdirs = [d.name for d in src_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"\\nSource '{src}' subfolders:\", subdirs)\n",
    "\n",
    "    for cls in expected_classes:\n",
    "        cls_norm = normalize(cls)\n",
    "        matched = None\n",
    "\n",
    "       \n",
    "         for s in subdirs:\n",
    "            if cls_norm in normalize(s):\n",
    "                matched = s\n",
    "                break\n",
    "\n",
    "        if matched is None:\n",
    "            print(f\"WARNING: class '{cls}' not found under '{src}'\")\n",
    "            continue\n",
    "\n",
    "        found_map.setdefault(src, {})[cls] = matched\n",
    "        cls_dir = src_dir / matched\n",
    "\n",
    "        for p in cls_dir.glob(\"*\"):\n",
    "            if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                filepaths.append(str(p))\n",
    "                labels.append(expected_classes.index(cls))\n",
    "\n",
    "print(\"\\nTotal images found:\", len(filepaths))\n",
    "from collections import Counter\n",
    "ctr = Counter([Path(p).parent.name for p in filepaths])\n",
    "print(\"Per-subfolder counts:\")\n",
    "for k, v in ctr.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "manifest = {\n",
    "    \"classes\": expected_classes,\n",
    "    \"sources_detected\": sources,\n",
    "    \"found_map\": found_map,\n",
    "    \"files\": filepaths,\n",
    "    \"labels\": labels\n",
    "}\n",
    "\n",
    "os.makedirs(\"/kaggle/working/simsiam_task4\", exist_ok=True)\n",
    "with open(\"/kaggle/working/simsiam_task4/manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f)\n",
    "print(\"\\nManifest saved to /kaggle/working/simsiam_task4/manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4 - Transforms (SimSiam two-view + eval transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:11:56.237754Z",
     "iopub.status.busy": "2025-12-05T08:11:56.237089Z",
     "iopub.status.idle": "2025-12-05T08:11:56.245377Z",
     "shell.execute_reply": "2025-12-05T08:11:56.244786Z",
     "shell.execute_reply.started": "2025-12-05T08:11:56.237728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "simsiam_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(RESOLUTION, scale=(0.2, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.02),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(int(RESOLUTION * 1.1)),\n",
    "    transforms.CenterCrop(RESOLUTION),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def aug_probe_image(path, n=6):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    outs = []\n",
    "    for _ in range(n):\n",
    "        timg = simsiam_transform(img)\n",
    "        # de-normalize for visualization\n",
    "        t = timg.numpy().transpose(1,2,0)\n",
    "        t = t * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])\n",
    "        t = np.clip(t, 0, 1)\n",
    "        outs.append((t*255).astype(np.uint8))\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5 - Dataset wrappers: TwoViewDataset + ManifestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:12:01.255707Z",
     "iopub.status.busy": "2025-12-05T08:12:01.255128Z",
     "iopub.status.idle": "2025-12-05T08:12:01.261788Z",
     "shell.execute_reply": "2025-12-05T08:12:01.261019Z",
     "shell.execute_reply.started": "2025-12-05T08:12:01.255684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TwoViewDataset(Dataset):\n",
    "    \"\"\"Returns two different augmented views of the same image (for SimSiam).\"\"\"\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        lbl = self.labels[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x1 = self.transform(img)\n",
    "        x2 = self.transform(img)\n",
    "        return x1, x2, lbl, p\n",
    "\n",
    "class ManifestDataset(Dataset):\n",
    "    \"\"\"Deterministic dataset for feature extraction and downstream training.\"\"\"\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        lbl = self.labels[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, lbl, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 6 - Create train/val/test splits and DataLoaders (deterministic split saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:12:10.373858Z",
     "iopub.status.busy": "2025-12-05T08:12:10.373575Z",
     "iopub.status.idle": "2025-12-05T08:12:10.402244Z",
     "shell.execute_reply": "2025-12-05T08:12:10.401364Z",
     "shell.execute_reply.started": "2025-12-05T08:12:10.373837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1296 Val: 144 Test: 360\n",
      "Split manifest saved to /kaggle/working/simsiam_task4/split_manifest.json\n"
     ]
    }
   ],
   "source": [
    "paths = manifest[\"files\"]\n",
    "labels = manifest[\"labels\"]\n",
    "classes = manifest[\"classes\"]\n",
    "\n",
    "# First split: fixed test set 20%\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    paths, labels, test_size=0.20, stratify=labels, random_state=SEED)\n",
    "\n",
    "# From train, carve out validation 10% of train\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.10, stratify=train_labels, random_state=SEED)\n",
    "\n",
    "print(\"Train:\", len(train_paths), \"Val:\", len(val_paths), \"Test:\", len(test_paths))\n",
    "\n",
    "split_manifest = {\n",
    "    \"classes\": classes,\n",
    "    \"train\": train_paths, \"train_labels\": train_labels,\n",
    "    \"val\": val_paths, \"val_labels\": val_labels,\n",
    "    \"test\": test_paths, \"test_labels\": test_labels\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"split_manifest.json\"), \"w\") as f:\n",
    "    json.dump(split_manifest, f)\n",
    "print(\"Split manifest saved to\", os.path.join(OUT_DIR, \"split_manifest.json\"))\n",
    "\n",
    "# DataLoaders for pretraining (two-view)\n",
    "train_dataset = TwoViewDataset(train_paths, train_labels, simsiam_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, drop_last=True)\n",
    "\n",
    "# DataLoaders for evaluation (use ManifestDataset)\n",
    "val_dataset = ManifestDataset(val_paths, val_labels, eval_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataset = ManifestDataset(test_paths, test_labels, eval_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 7 - SimSiam model definition (encoder, projector, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:12:18.646785Z",
     "iopub.status.busy": "2025-12-05T08:12:18.646118Z",
     "iopub.status.idle": "2025-12-05T08:12:18.654772Z",
     "shell.execute_reply": "2025-12-05T08:12:18.653928Z",
     "shell.execute_reply.started": "2025-12-05T08:12:18.646762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\", pretrained=False, proj_hidden=2048, pred_hidden=512, out_dim=512):\n",
    "        super().__init__()\n",
    "        # backbone\n",
    "        if backbone == \"resnet18\":\n",
    "            base = models.resnet18(pretrained=pretrained)\n",
    "            feat_dim = 512\n",
    "        elif backbone == \"resnet50\":\n",
    "            base = models.resnet50(pretrained=pretrained)\n",
    "            feat_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(\"backbone must be resnet18 or resnet50\")\n",
    "        \n",
    "        modules = list(base.children())[:-1]\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, proj_hidden),\n",
    "            nn.BatchNorm1d(proj_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(proj_hidden, proj_hidden),\n",
    "            nn.BatchNorm1d(proj_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(proj_hidden, out_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(out_dim, pred_hidden),\n",
    "            nn.BatchNorm1d(pred_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pred_hidden, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        \n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        h1 = self.forward_backbone(x1)\n",
    "        h2 = self.forward_backbone(x2)\n",
    "        z1 = self.projector(h1)\n",
    "        z2 = self.projector(h2)\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        \n",
    "        return p1, p2, z1.detach(), z2.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 8 - Loss function (negative cosine similarity) & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:12:24.657042Z",
     "iopub.status.busy": "2025-12-05T08:12:24.656281Z",
     "iopub.status.idle": "2025-12-05T08:12:24.661277Z",
     "shell.execute_reply": "2025-12-05T08:12:24.660545Z",
     "shell.execute_reply.started": "2025-12-05T08:12:24.657017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def negative_cosine_similarity(p, z):\n",
    "\n",
    "    p = nn.functional.normalize(p, dim=1)\n",
    "    z = nn.functional.normalize(z, dim=1)\n",
    "    return - (p * z).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "    print(\"Saved checkpoint:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 9 - Pretraining loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:20:26.189444Z",
     "iopub.status.busy": "2025-11-23T19:20:26.188907Z",
     "iopub.status.idle": "2025-11-24T05:49:19.686304Z",
     "shell.execute_reply": "2025-11-24T05:49:19.685492Z",
     "shell.execute_reply.started": "2025-11-23T19:20:26.189421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pretraining for 100 epochs (from epoch 0 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 1/100: 100%|██████████| 20/20 [06:57<00:00, 20.89s/it, loss=-0.1287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Avg loss: -0.1287\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 2/100: 100%|██████████| 20/20 [06:21<00:00, 19.07s/it, loss=-0.4303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Avg loss: -0.4303\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 3/100: 100%|██████████| 20/20 [06:14<00:00, 18.73s/it, loss=-0.5787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Avg loss: -0.5787\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 4/100: 100%|██████████| 20/20 [06:10<00:00, 18.51s/it, loss=-0.6860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Avg loss: -0.6860\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 5/100: 100%|██████████| 20/20 [06:19<00:00, 18.96s/it, loss=-0.7681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Avg loss: -0.7681\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 6/100: 100%|██████████| 20/20 [06:10<00:00, 18.54s/it, loss=-0.8111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Avg loss: -0.8111\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 7/100: 100%|██████████| 20/20 [06:13<00:00, 18.67s/it, loss=-0.8327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Avg loss: -0.8327\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 8/100: 100%|██████████| 20/20 [06:14<00:00, 18.72s/it, loss=-0.8515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished. Avg loss: -0.8515\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 9/100: 100%|██████████| 20/20 [06:17<00:00, 18.88s/it, loss=-0.8444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished. Avg loss: -0.8444\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10/100: 100%|██████████| 20/20 [06:17<00:00, 18.89s/it, loss=-0.8696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished. Avg loss: -0.8696\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 11/100: 100%|██████████| 20/20 [06:19<00:00, 18.98s/it, loss=-0.8581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished. Avg loss: -0.8581\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 12/100: 100%|██████████| 20/20 [06:14<00:00, 18.72s/it, loss=-0.8753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished. Avg loss: -0.8753\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 13/100: 100%|██████████| 20/20 [06:06<00:00, 18.35s/it, loss=-0.8759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished. Avg loss: -0.8759\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 14/100: 100%|██████████| 20/20 [06:13<00:00, 18.68s/it, loss=-0.8736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished. Avg loss: -0.8736\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 15/100: 100%|██████████| 20/20 [06:13<00:00, 18.65s/it, loss=-0.8942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished. Avg loss: -0.8942\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 16/100: 100%|██████████| 20/20 [06:19<00:00, 18.98s/it, loss=-0.8858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 finished. Avg loss: -0.8858\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 17/100: 100%|██████████| 20/20 [06:13<00:00, 18.67s/it, loss=-0.8934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 finished. Avg loss: -0.8934\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 18/100: 100%|██████████| 20/20 [05:59<00:00, 18.00s/it, loss=-0.8937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 finished. Avg loss: -0.8937\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 19/100: 100%|██████████| 20/20 [06:17<00:00, 18.90s/it, loss=-0.8942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 finished. Avg loss: -0.8942\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 20/100: 100%|██████████| 20/20 [06:19<00:00, 18.97s/it, loss=-0.9003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 finished. Avg loss: -0.9003\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 21/100: 100%|██████████| 20/20 [06:11<00:00, 18.59s/it, loss=-0.8986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 finished. Avg loss: -0.8986\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 22/100: 100%|██████████| 20/20 [06:11<00:00, 18.58s/it, loss=-0.8984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 finished. Avg loss: -0.8984\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 23/100: 100%|██████████| 20/20 [06:20<00:00, 19.04s/it, loss=-0.8972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 finished. Avg loss: -0.8972\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 24/100: 100%|██████████| 20/20 [06:18<00:00, 18.91s/it, loss=-0.8948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 finished. Avg loss: -0.8948\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 25/100: 100%|██████████| 20/20 [06:03<00:00, 18.16s/it, loss=-0.8922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 finished. Avg loss: -0.8922\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 26/100: 100%|██████████| 20/20 [06:19<00:00, 18.97s/it, loss=-0.8974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 finished. Avg loss: -0.8974\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 27/100: 100%|██████████| 20/20 [06:16<00:00, 18.80s/it, loss=-0.8999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 finished. Avg loss: -0.8999\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 28/100: 100%|██████████| 20/20 [06:10<00:00, 18.54s/it, loss=-0.9013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 finished. Avg loss: -0.9013\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 29/100: 100%|██████████| 20/20 [06:20<00:00, 19.02s/it, loss=-0.9023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 finished. Avg loss: -0.9023\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 30/100: 100%|██████████| 20/20 [06:21<00:00, 19.08s/it, loss=-0.9070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 finished. Avg loss: -0.9070\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 31/100: 100%|██████████| 20/20 [06:21<00:00, 19.06s/it, loss=-0.9009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 finished. Avg loss: -0.9009\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 32/100: 100%|██████████| 20/20 [06:28<00:00, 19.40s/it, loss=-0.9014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 finished. Avg loss: -0.9014\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 33/100: 100%|██████████| 20/20 [06:12<00:00, 18.64s/it, loss=-0.8989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 finished. Avg loss: -0.8989\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 34/100: 100%|██████████| 20/20 [06:18<00:00, 18.91s/it, loss=-0.9083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 finished. Avg loss: -0.9083\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 35/100: 100%|██████████| 20/20 [06:20<00:00, 19.03s/it, loss=-0.9056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 finished. Avg loss: -0.9056\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 36/100: 100%|██████████| 20/20 [06:13<00:00, 18.67s/it, loss=-0.9021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 finished. Avg loss: -0.9021\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 37/100: 100%|██████████| 20/20 [06:19<00:00, 18.96s/it, loss=-0.9027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 finished. Avg loss: -0.9027\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 38/100: 100%|██████████| 20/20 [06:21<00:00, 19.06s/it, loss=-0.9024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 finished. Avg loss: -0.9024\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 39/100: 100%|██████████| 20/20 [06:17<00:00, 18.87s/it, loss=-0.9041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 finished. Avg loss: -0.9041\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 40/100: 100%|██████████| 20/20 [06:15<00:00, 18.77s/it, loss=-0.9015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 finished. Avg loss: -0.9015\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 41/100: 100%|██████████| 20/20 [06:12<00:00, 18.61s/it, loss=-0.8954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 finished. Avg loss: -0.8954\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 42/100: 100%|██████████| 20/20 [06:16<00:00, 18.84s/it, loss=-0.9000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 finished. Avg loss: -0.9000\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 43/100: 100%|██████████| 20/20 [06:20<00:00, 19.05s/it, loss=-0.9058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 finished. Avg loss: -0.9058\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 44/100: 100%|██████████| 20/20 [06:22<00:00, 19.13s/it, loss=-0.9043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 finished. Avg loss: -0.9043\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 45/100: 100%|██████████| 20/20 [06:25<00:00, 19.29s/it, loss=-0.9010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 finished. Avg loss: -0.9010\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 46/100: 100%|██████████| 20/20 [06:22<00:00, 19.13s/it, loss=-0.9076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 finished. Avg loss: -0.9076\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 47/100: 100%|██████████| 20/20 [06:16<00:00, 18.82s/it, loss=-0.9098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 finished. Avg loss: -0.9098\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 48/100: 100%|██████████| 20/20 [06:14<00:00, 18.73s/it, loss=-0.9045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 finished. Avg loss: -0.9045\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 49/100: 100%|██████████| 20/20 [06:23<00:00, 19.17s/it, loss=-0.9105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 finished. Avg loss: -0.9105\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 50/100: 100%|██████████| 20/20 [06:11<00:00, 18.60s/it, loss=-0.9072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 finished. Avg loss: -0.9072\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 51/100: 100%|██████████| 20/20 [06:22<00:00, 19.13s/it, loss=-0.9112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 finished. Avg loss: -0.9112\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 52/100: 100%|██████████| 20/20 [06:18<00:00, 18.93s/it, loss=-0.9114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 finished. Avg loss: -0.9114\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 53/100: 100%|██████████| 20/20 [06:14<00:00, 18.74s/it, loss=-0.9047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 finished. Avg loss: -0.9047\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 54/100: 100%|██████████| 20/20 [06:29<00:00, 19.47s/it, loss=-0.8987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 finished. Avg loss: -0.8987\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 55/100: 100%|██████████| 20/20 [06:21<00:00, 19.05s/it, loss=-0.9183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 finished. Avg loss: -0.9183\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 56/100: 100%|██████████| 20/20 [06:15<00:00, 18.76s/it, loss=-0.9190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 finished. Avg loss: -0.9190\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 57/100: 100%|██████████| 20/20 [06:17<00:00, 18.88s/it, loss=-0.9139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 finished. Avg loss: -0.9139\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 58/100: 100%|██████████| 20/20 [06:24<00:00, 19.23s/it, loss=-0.9146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 finished. Avg loss: -0.9146\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 59/100: 100%|██████████| 20/20 [06:18<00:00, 18.92s/it, loss=-0.9131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 finished. Avg loss: -0.9131\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 60/100: 100%|██████████| 20/20 [06:21<00:00, 19.06s/it, loss=-0.9122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 finished. Avg loss: -0.9122\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 61/100: 100%|██████████| 20/20 [06:19<00:00, 18.98s/it, loss=-0.9176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 finished. Avg loss: -0.9176\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 62/100: 100%|██████████| 20/20 [06:19<00:00, 18.97s/it, loss=-0.9130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 finished. Avg loss: -0.9130\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 63/100: 100%|██████████| 20/20 [06:18<00:00, 18.93s/it, loss=-0.9147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 finished. Avg loss: -0.9147\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 64/100: 100%|██████████| 20/20 [06:21<00:00, 19.07s/it, loss=-0.9130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 finished. Avg loss: -0.9130\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 65/100: 100%|██████████| 20/20 [06:19<00:00, 18.99s/it, loss=-0.9130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 finished. Avg loss: -0.9130\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 66/100: 100%|██████████| 20/20 [06:20<00:00, 19.02s/it, loss=-0.9187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 finished. Avg loss: -0.9187\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 67/100: 100%|██████████| 20/20 [06:19<00:00, 18.97s/it, loss=-0.9150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 finished. Avg loss: -0.9150\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 68/100: 100%|██████████| 20/20 [06:05<00:00, 18.28s/it, loss=-0.9201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 finished. Avg loss: -0.9201\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 69/100: 100%|██████████| 20/20 [06:22<00:00, 19.14s/it, loss=-0.9135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 finished. Avg loss: -0.9135\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 70/100: 100%|██████████| 20/20 [06:23<00:00, 19.18s/it, loss=-0.9123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 finished. Avg loss: -0.9123\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 71/100: 100%|██████████| 20/20 [06:22<00:00, 19.10s/it, loss=-0.9177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 finished. Avg loss: -0.9177\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 72/100: 100%|██████████| 20/20 [06:20<00:00, 19.03s/it, loss=-0.9154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 finished. Avg loss: -0.9154\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 73/100: 100%|██████████| 20/20 [06:24<00:00, 19.22s/it, loss=-0.9192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 finished. Avg loss: -0.9192\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 74/100: 100%|██████████| 20/20 [06:22<00:00, 19.13s/it, loss=-0.9154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 finished. Avg loss: -0.9154\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 75/100: 100%|██████████| 20/20 [06:21<00:00, 19.06s/it, loss=-0.9158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 finished. Avg loss: -0.9158\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 76/100: 100%|██████████| 20/20 [06:22<00:00, 19.13s/it, loss=-0.9128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 finished. Avg loss: -0.9128\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 77/100: 100%|██████████| 20/20 [06:13<00:00, 18.69s/it, loss=-0.9191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 finished. Avg loss: -0.9191\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 78/100: 100%|██████████| 20/20 [06:13<00:00, 18.65s/it, loss=-0.9230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 finished. Avg loss: -0.9230\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 79/100: 100%|██████████| 20/20 [06:17<00:00, 18.88s/it, loss=-0.9172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 finished. Avg loss: -0.9172\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 80/100: 100%|██████████| 20/20 [06:15<00:00, 18.77s/it, loss=-0.9178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 finished. Avg loss: -0.9178\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 81/100: 100%|██████████| 20/20 [06:12<00:00, 18.60s/it, loss=-0.9160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 finished. Avg loss: -0.9160\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 82/100: 100%|██████████| 20/20 [06:21<00:00, 19.06s/it, loss=-0.9195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 finished. Avg loss: -0.9195\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 83/100: 100%|██████████| 20/20 [06:15<00:00, 18.76s/it, loss=-0.9179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 finished. Avg loss: -0.9179\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 84/100: 100%|██████████| 20/20 [06:18<00:00, 18.92s/it, loss=-0.9208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 finished. Avg loss: -0.9208\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 85/100: 100%|██████████| 20/20 [05:50<00:00, 17.52s/it, loss=-0.9239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 finished. Avg loss: -0.9239\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 86/100: 100%|██████████| 20/20 [06:19<00:00, 18.96s/it, loss=-0.9157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 finished. Avg loss: -0.9157\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 87/100: 100%|██████████| 20/20 [06:20<00:00, 19.01s/it, loss=-0.9111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 finished. Avg loss: -0.9111\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 88/100: 100%|██████████| 20/20 [06:11<00:00, 18.60s/it, loss=-0.9194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 finished. Avg loss: -0.9194\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 89/100: 100%|██████████| 20/20 [06:14<00:00, 18.72s/it, loss=-0.9169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 finished. Avg loss: -0.9169\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 90/100: 100%|██████████| 20/20 [06:14<00:00, 18.71s/it, loss=-0.9176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 finished. Avg loss: -0.9176\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 91/100: 100%|██████████| 20/20 [06:17<00:00, 18.87s/it, loss=-0.9189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 finished. Avg loss: -0.9189\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 92/100: 100%|██████████| 20/20 [05:32<00:00, 16.64s/it, loss=-0.9194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 finished. Avg loss: -0.9194\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 93/100: 100%|██████████| 20/20 [06:16<00:00, 18.81s/it, loss=-0.9203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 finished. Avg loss: -0.9203\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 94/100: 100%|██████████| 20/20 [06:14<00:00, 18.73s/it, loss=-0.9212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 finished. Avg loss: -0.9212\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 95/100: 100%|██████████| 20/20 [06:16<00:00, 18.82s/it, loss=-0.9223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 finished. Avg loss: -0.9223\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 96/100: 100%|██████████| 20/20 [06:11<00:00, 18.57s/it, loss=-0.9198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 finished. Avg loss: -0.9198\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 97/100: 100%|██████████| 20/20 [06:14<00:00, 18.74s/it, loss=-0.9226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 finished. Avg loss: -0.9226\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 98/100: 100%|██████████| 20/20 [06:11<00:00, 18.56s/it, loss=-0.9148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 finished. Avg loss: -0.9148\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 99/100: 100%|██████████| 20/20 [06:18<00:00, 18.95s/it, loss=-0.9190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 finished. Avg loss: -0.9190\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 100/100: 100%|██████████| 20/20 [06:15<00:00, 18.76s/it, loss=-0.9217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 finished. Avg loss: -0.9217\n",
      "Saved checkpoint: /kaggle/working/simsiam_task4/simsiam_latest.pth\n",
      "Pretraining complete. Encoder saved to /kaggle/working/simsiam_task4/simsiam_encoder.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.03 * (BATCH_SIZE / 256)\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "\n",
    "model = SimSiam(backbone=BACKBONE).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=PRETRAIN_EPOCHS)\n",
    "\n",
    "\n",
    "latest_ckpt = os.path.join(OUT_DIR, \"simsiam_latest.pth\")\n",
    "best_ckpt = os.path.join(OUT_DIR, \"simsiam_best_linearprobe.pth\")\n",
    "encoder_outpath = os.path.join(OUT_DIR, \"simsiam_encoder.pth\")\n",
    "\n",
    "\n",
    "def extract_features_from_encoder(encoder, paths_list, transform, batch_size=64):\n",
    "    encoder.eval()\n",
    "    ds = ManifestDataset(paths_list, [0]*len(paths_list), transform=transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, _ in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            h = encoder(imgs).view(imgs.size(0), -1).cpu().numpy()\n",
    "            feats.append(h)\n",
    "    feats = np.vstack(feats)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def quick_linear_probe(encoder, train_paths, train_labels, val_paths, val_labels, transform, max_samples=500):\n",
    "\n",
    "    tpaths = train_paths[:max_samples]; tlabels = train_labels[:max_samples]\n",
    "    train_feats = extract_features_from_encoder(encoder, tpaths, transform)\n",
    "    val_feats = extract_features_from_encoder(encoder, val_paths, transform)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(train_feats, tlabels)\n",
    "    preds = clf.predict(val_feats)\n",
    "    acc = accuracy_score(val_labels, preds)\n",
    "    return acc\n",
    "\n",
    "# Main training loop\n",
    "print(\"Starting pretraining for\", PRETRAIN_EPOCHS, \"epochs (from epoch\", start_epoch, \")\")\n",
    "for epoch in range(start_epoch, PRETRAIN_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    loop = tqdm(train_loader, desc=f\"Pretrain Epoch {epoch+1}/{PRETRAIN_EPOCHS}\")\n",
    "    for x1, x2, lbl, _ in loop:\n",
    "        x1 = x1.to(DEVICE); x2 = x2.to(DEVICE)\n",
    "        p1, p2, z1, z2 = model(x1, x2)\n",
    "        loss = 0.5 * negative_cosine_similarity(p1, z2) + 0.5 * negative_cosine_similarity(p2, z1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        loop.set_postfix(loss=f\"{np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = float(np.mean(epoch_losses))\n",
    "    print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    ck = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"manifest\": split_manifest\n",
    "    }\n",
    "    save_checkpoint(ck, latest_ckpt)\n",
    "\n",
    "\n",
    "torch.save({\"encoder_state_dict\": model.encoder.state_dict(), \"feat_dim\": model.feat_dim},\n",
    "           encoder_outpath)\n",
    "print(\"Pretraining complete. Encoder saved to\", encoder_outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 4.2) CELL 10 - Feature extraction (frozen encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:13:10.332070Z",
     "iopub.status.busy": "2025-12-05T08:13:10.331716Z",
     "iopub.status.idle": "2025-12-05T08:21:22.431694Z",
     "shell.execute_reply": "2025-12-05T08:21:22.430877Z",
     "shell.execute_reply.started": "2025-12-05T08:13:10.332037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train features: (1296, 512) to /kaggle/working/simsiam_task4/train_feats.npy\n",
      "Saved val features: (144, 512) to /kaggle/working/simsiam_task4/val_feats.npy\n",
      "Saved test features: (360, 512) to /kaggle/working/simsiam_task4/test_feats.npy\n"
     ]
    }
   ],
   "source": [
    "enc_ckpt = os.path.join(OUT_DIR, \"simsiam_encoder.pth\")\n",
    "if not os.path.exists(enc_ckpt):\n",
    "    \n",
    "    if 'model' in globals() and hasattr(model, \"encoder\"):\n",
    "        encoder = model.encoder\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Encoder checkpoint not found and model not in memory.\")\n",
    "else:\n",
    "    d = torch.load(enc_ckpt, map_location=DEVICE)\n",
    "    encoder = SimSiam(backbone=BACKBONE).encoder\n",
    "    encoder.load_state_dict(d[\"encoder_state_dict\"])\n",
    "encoder = encoder.to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "\n",
    "def extract_and_save(paths_list, labels_list, split_name):\n",
    "    ds = ManifestDataset(paths_list, labels_list, eval_transform)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    feats = []\n",
    "    files = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls, ps in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            h = encoder(imgs).view(imgs.size(0), -1).cpu().numpy()\n",
    "            feats.append(h)\n",
    "            files.extend(ps)\n",
    "    feats = np.vstack(feats)\n",
    "    np.save(os.path.join(OUT_DIR, f\"{split_name}_feats.npy\"), feats)\n",
    "    np.save(os.path.join(OUT_DIR, f\"{split_name}_labels.npy\"), np.array(labels_list))\n",
    "    print(f\"Saved {split_name} features: {feats.shape} to {OUT_DIR}/{split_name}_feats.npy\")\n",
    "    return feats\n",
    "\n",
    "train_feats = extract_and_save(train_paths, train_labels, \"train\")\n",
    "val_feats = extract_and_save(val_paths, val_labels, \"val\")\n",
    "test_feats = extract_and_save(test_paths, test_labels, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 11 - Linear probe + shallow heads evaluations (train classifiers on frozen features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:32:32.581415Z",
     "iopub.status.busy": "2025-12-05T08:32:32.580678Z",
     "iopub.status.idle": "2025-12-05T08:32:45.764310Z",
     "shell.execute_reply": "2025-12-05T08:32:45.763236Z",
     "shell.execute_reply.started": "2025-12-05T08:32:32.581384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: LogisticRegression\n",
      " LogisticRegression val_acc: 0.8333 test_acc: 0.8167\n",
      "Training: SVM_RBF\n",
      " SVM_RBF val_acc: 0.7917 test_acc: 0.7806\n",
      "Training: RandomForest\n",
      " RandomForest val_acc: 0.8403 test_acc: 0.7972\n",
      "Training: DecisionTree\n",
      " DecisionTree val_acc: 0.7431 test_acc: 0.6778\n",
      "Training: MLP\n",
      " MLP val_acc: 0.8750 test_acc: 0.8528\n",
      "Probe results saved to /kaggle/working/simsiam_task4/probe_results.json\n"
     ]
    }
   ],
   "source": [
    "train_feats = np.load(os.path.join(OUT_DIR, \"train_feats.npy\"))\n",
    "train_lbls = np.load(os.path.join(OUT_DIR, \"train_labels.npy\"))\n",
    "val_feats = np.load(os.path.join(OUT_DIR, \"val_feats.npy\"))\n",
    "val_lbls = np.load(os.path.join(OUT_DIR, \"val_labels.npy\"))\n",
    "test_feats = np.load(os.path.join(OUT_DIR, \"test_feats.npy\"))\n",
    "test_lbls = np.load(os.path.join(OUT_DIR, \"test_labels.npy\"))\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000),\n",
    "    \"SVM_RBF\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(512,), max_iter=500)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"Training:\", name)\n",
    "    clf.fit(train_feats, train_lbls)\n",
    "    val_pred = clf.predict(val_feats)\n",
    "    val_acc = accuracy_score(val_lbls, val_pred)\n",
    "    test_pred = clf.predict(test_feats)\n",
    "    test_acc = accuracy_score(test_lbls, test_pred)\n",
    "    print(f\" {name} val_acc: {val_acc:.4f} test_acc: {test_acc:.4f}\")\n",
    "    results[name] = {\"val_acc\": float(val_acc), \"test_acc\": float(test_acc)}\n",
    "    joblib.dump(clf, os.path.join(OUT_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"probe_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Probe results saved to\", os.path.join(OUT_DIR, \"probe_results.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 12 - Full fine-tune: attach classification head and fine-tune entire encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:32:52.754937Z",
     "iopub.status.busy": "2025-12-05T08:32:52.754629Z",
     "iopub.status.idle": "2025-12-05T18:20:22.864082Z",
     "shell.execute_reply": "2025-12-05T18:20:22.863222Z",
     "shell.execute_reply.started": "2025-12-05T08:32:52.754915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1/50: 100%|██████████| 21/21 [10:33<00:00, 30.17s/it, train_loss=0.8706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1/50 - train_loss: 0.8706 val_acc: 0.3889\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 2/50: 100%|██████████| 21/21 [10:34<00:00, 30.23s/it, train_loss=0.6193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 2/50 - train_loss: 0.6193 val_acc: 0.5694\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 3/50: 100%|██████████| 21/21 [10:30<00:00, 30.02s/it, train_loss=0.7477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 3/50 - train_loss: 0.7477 val_acc: 0.7431\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 4/50: 100%|██████████| 21/21 [10:27<00:00, 29.87s/it, train_loss=0.4576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 4/50 - train_loss: 0.4576 val_acc: 0.7778\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 5/50: 100%|██████████| 21/21 [10:29<00:00, 29.98s/it, train_loss=0.4598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 5/50 - train_loss: 0.4598 val_acc: 0.8681\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 6/50: 100%|██████████| 21/21 [10:29<00:00, 29.97s/it, train_loss=0.3948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 6/50 - train_loss: 0.3948 val_acc: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 7/50: 100%|██████████| 21/21 [10:37<00:00, 30.33s/it, train_loss=0.3207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 7/50 - train_loss: 0.3207 val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 8/50: 100%|██████████| 21/21 [10:31<00:00, 30.07s/it, train_loss=0.3516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 8/50 - train_loss: 0.3516 val_acc: 0.8958\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 9/50: 100%|██████████| 21/21 [10:33<00:00, 30.19s/it, train_loss=0.3168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 9/50 - train_loss: 0.3168 val_acc: 0.8681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 10/50: 100%|██████████| 21/21 [10:34<00:00, 30.19s/it, train_loss=0.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 10/50 - train_loss: 0.2560 val_acc: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 11/50: 100%|██████████| 21/21 [10:39<00:00, 30.45s/it, train_loss=0.2551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 11/50 - train_loss: 0.2551 val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 12/50: 100%|██████████| 21/21 [10:44<00:00, 30.70s/it, train_loss=0.2649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 12/50 - train_loss: 0.2649 val_acc: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 13/50: 100%|██████████| 21/21 [10:29<00:00, 29.99s/it, train_loss=0.2212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 13/50 - train_loss: 0.2212 val_acc: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 14/50: 100%|██████████| 21/21 [10:25<00:00, 29.77s/it, train_loss=0.2540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 14/50 - train_loss: 0.2540 val_acc: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 15/50: 100%|██████████| 21/21 [10:26<00:00, 29.85s/it, train_loss=0.2133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 15/50 - train_loss: 0.2133 val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 16/50: 100%|██████████| 21/21 [10:37<00:00, 30.38s/it, train_loss=0.1158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 16/50 - train_loss: 0.1158 val_acc: 0.9236\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 17/50: 100%|██████████| 21/21 [10:22<00:00, 29.66s/it, train_loss=0.0835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 17/50 - train_loss: 0.0835 val_acc: 0.9306\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 18/50: 100%|██████████| 21/21 [10:35<00:00, 30.26s/it, train_loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 18/50 - train_loss: 0.0748 val_acc: 0.9375\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 19/50: 100%|██████████| 21/21 [10:25<00:00, 29.78s/it, train_loss=0.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 19/50 - train_loss: 0.0656 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 20/50: 100%|██████████| 21/21 [10:23<00:00, 29.68s/it, train_loss=0.0606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 20/50 - train_loss: 0.0606 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 21/50: 100%|██████████| 21/21 [10:29<00:00, 29.99s/it, train_loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 21/50 - train_loss: 0.0553 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 22/50: 100%|██████████| 21/21 [10:24<00:00, 29.73s/it, train_loss=0.0635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 22/50 - train_loss: 0.0635 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 23/50: 100%|██████████| 21/21 [10:28<00:00, 29.93s/it, train_loss=0.0465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 23/50 - train_loss: 0.0465 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 24/50: 100%|██████████| 21/21 [10:34<00:00, 30.22s/it, train_loss=0.0815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 24/50 - train_loss: 0.0815 val_acc: 0.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 25/50: 100%|██████████| 21/21 [10:33<00:00, 30.15s/it, train_loss=0.0767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 25/50 - train_loss: 0.0767 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 26/50: 100%|██████████| 21/21 [10:25<00:00, 29.81s/it, train_loss=0.0469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 26/50 - train_loss: 0.0469 val_acc: 0.9444\n",
      "Saved best fine-tune checkpoint: /kaggle/working/simsiam_task4/finetune_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 27/50: 100%|██████████| 21/21 [10:33<00:00, 30.17s/it, train_loss=0.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 27/50 - train_loss: 0.0623 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 28/50: 100%|██████████| 21/21 [10:34<00:00, 30.24s/it, train_loss=0.0466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 28/50 - train_loss: 0.0466 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 29/50: 100%|██████████| 21/21 [10:31<00:00, 30.08s/it, train_loss=0.0395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 29/50 - train_loss: 0.0395 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 30/50: 100%|██████████| 21/21 [10:35<00:00, 30.24s/it, train_loss=0.0448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 30/50 - train_loss: 0.0448 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 31/50: 100%|██████████| 21/21 [10:38<00:00, 30.40s/it, train_loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 31/50 - train_loss: 0.0296 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 32/50: 100%|██████████| 21/21 [10:43<00:00, 30.64s/it, train_loss=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 32/50 - train_loss: 0.0322 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 33/50: 100%|██████████| 21/21 [10:29<00:00, 29.96s/it, train_loss=0.0306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 33/50 - train_loss: 0.0306 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 34/50: 100%|██████████| 21/21 [10:34<00:00, 30.20s/it, train_loss=0.0329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 34/50 - train_loss: 0.0329 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 35/50: 100%|██████████| 21/21 [10:33<00:00, 30.17s/it, train_loss=0.0472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 35/50 - train_loss: 0.0472 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 36/50: 100%|██████████| 21/21 [10:37<00:00, 30.38s/it, train_loss=0.0243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 36/50 - train_loss: 0.0243 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 37/50: 100%|██████████| 21/21 [10:33<00:00, 30.15s/it, train_loss=0.0286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 37/50 - train_loss: 0.0286 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 38/50: 100%|██████████| 21/21 [10:38<00:00, 30.40s/it, train_loss=0.0308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 38/50 - train_loss: 0.0308 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 39/50: 100%|██████████| 21/21 [10:33<00:00, 30.15s/it, train_loss=0.0283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 39/50 - train_loss: 0.0283 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 40/50: 100%|██████████| 21/21 [10:43<00:00, 30.66s/it, train_loss=0.0349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 40/50 - train_loss: 0.0349 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 41/50: 100%|██████████| 21/21 [10:31<00:00, 30.06s/it, train_loss=0.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 41/50 - train_loss: 0.0262 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 42/50: 100%|██████████| 21/21 [10:40<00:00, 30.52s/it, train_loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 42/50 - train_loss: 0.0232 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 43/50: 100%|██████████| 21/21 [10:38<00:00, 30.38s/it, train_loss=0.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 43/50 - train_loss: 0.0282 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 44/50: 100%|██████████| 21/21 [10:30<00:00, 30.01s/it, train_loss=0.0302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 44/50 - train_loss: 0.0302 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 45/50: 100%|██████████| 21/21 [10:23<00:00, 29.69s/it, train_loss=0.0270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 45/50 - train_loss: 0.0270 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 46/50: 100%|██████████| 21/21 [10:35<00:00, 30.27s/it, train_loss=0.0399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 46/50 - train_loss: 0.0399 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 47/50: 100%|██████████| 21/21 [10:31<00:00, 30.05s/it, train_loss=0.0271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 47/50 - train_loss: 0.0271 val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 48/50: 100%|██████████| 21/21 [10:33<00:00, 30.19s/it, train_loss=0.0240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 48/50 - train_loss: 0.0240 val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 49/50: 100%|██████████| 21/21 [10:40<00:00, 30.49s/it, train_loss=0.0271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 49/50 - train_loss: 0.0271 val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 50/50: 100%|██████████| 21/21 [10:31<00:00, 30.09s/it, train_loss=0.0240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 50/50 - train_loss: 0.0240 val_acc: 0.9375\n",
      "Fine-tune complete. Best val acc: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "ft_num_workers = 0\n",
    "pin_memory = True if torch.cuda.is_available() else False\n",
    "resume_ckpt_path = os.path.join(OUT_DIR, \"finetune_resume.pth\")\n",
    "best_ckpt_path = os.path.join(OUT_DIR, \"finetune_best.pth\")\n",
    "save_every_epoch = True\n",
    "\n",
    "\n",
    "if 'encoder' not in globals() or encoder is None:\n",
    "    enc_candidates = [\n",
    "        os.path.join(OUT_DIR, \"simsiam_encoder_memory.pth\"),\n",
    "        os.path.join(OUT_DIR, \"simsiam_encoder.pth\")\n",
    "    ]\n",
    "    found = None\n",
    "    for p in enc_candidates:\n",
    "        if os.path.exists(p):\n",
    "            found = p\n",
    "            break\n",
    "    if found is None:\n",
    "        raise FileNotFoundError(\"Encoder checkpoint not found in OUT_DIR. Run pretraining or restore archive.\")\n",
    "    enc_ck = torch.load(found, map_location=\"cpu\", weights_only=False)\n",
    "    BACKBONE = globals().get(\"BACKBONE\", \"resnet18\")\n",
    "    tmp_model = SimSiam(backbone=BACKBONE)\n",
    "    tmp_model.encoder.load_state_dict(enc_ck[\"encoder_state_dict\"])\n",
    "    encoder = tmp_model.encoder\n",
    "    del tmp_model\n",
    "\n",
    "\n",
    "encoder = encoder.to(DEVICE)\n",
    "\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, encoder, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).view(x.size(0), -1)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 3, RESOLUTION, RESOLUTION).to(DEVICE)\n",
    "    try:\n",
    "        out = encoder(dummy).view(1, -1)\n",
    "        feat_dim = out.shape[1]\n",
    "    except Exception:\n",
    "        # fallback to known dims\n",
    "        feat_dim = 512 if BACKBONE == \"resnet18\" else 2048\n",
    "\n",
    "num_classes = len(classes)\n",
    "ft_model = FineTuneClassifier(encoder, feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "\n",
    "ft_train_transform = transforms.Compose([\n",
    "    transforms.Resize(int(RESOLUTION*1.1)),\n",
    "    transforms.CenterCrop(RESOLUTION),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "ft_train_ds = ManifestDataset(train_paths, train_labels, ft_train_transform)\n",
    "ft_val_ds = ManifestDataset(val_paths, val_labels, eval_transform)\n",
    "\n",
    "ft_train_loader = DataLoader(ft_train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                             num_workers=ft_num_workers, pin_memory=pin_memory)\n",
    "ft_val_loader   = DataLoader(ft_val_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=ft_num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "ft_optimizer = optim.SGD(ft_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "ft_scheduler = optim.lr_scheduler.StepLR(ft_optimizer, step_size=15, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_acc = 0.0\n",
    "if os.path.exists(resume_ckpt_path):\n",
    "    try:\n",
    "        ck = torch.load(resume_ckpt_path, map_location=DEVICE, weights_only=False)\n",
    "        ft_model.load_state_dict(ck[\"model_state\"])\n",
    "        ft_optimizer.load_state_dict(ck[\"optimizer_state\"])\n",
    "        if \"scheduler_state\" in ck:\n",
    "            try:\n",
    "                ft_scheduler.load_state_dict(ck[\"scheduler_state\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        start_epoch = ck.get(\"epoch\", 0) + 1\n",
    "        best_val_acc = ck.get(\"val_acc\", 0.0)\n",
    "        print(f\"Resumed fine-tune from resume checkpoint at epoch {start_epoch} (best val {best_val_acc:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not resume from resume checkpoint:\", e)\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, FINETUNE_EPOCHS):\n",
    "        ft_model.train()\n",
    "        losses = []\n",
    "        loop = tqdm(ft_train_loader, desc=f\"Fine-tune Epoch {epoch+1}/{FINETUNE_EPOCHS}\")\n",
    "        for imgs, labels_batch, _ in loop:\n",
    "            imgs = imgs.to(DEVICE, non_blocking=pin_memory)\n",
    "            labels_batch = labels_batch.to(DEVICE, non_blocking=pin_memory)\n",
    "            logits = ft_model(imgs)\n",
    "            loss = criterion(logits, labels_batch)\n",
    "            ft_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            ft_optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            loop.set_postfix(train_loss=f\"{np.mean(losses):.4f}\")\n",
    "\n",
    "        ft_scheduler.step()\n",
    "\n",
    "\n",
    "        ft_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels_batch, _ in ft_val_loader:\n",
    "                imgs = imgs.to(DEVICE, non_blocking=pin_memory)\n",
    "                logits = ft_model(imgs)\n",
    "                preds = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels_batch.numpy())\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Fine-tune Epoch {epoch+1}/{FINETUNE_EPOCHS} - train_loss: {np.mean(losses):.4f} val_acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": ft_model.state_dict(),\n",
    "                \"optimizer_state\": ft_optimizer.state_dict(),\n",
    "                \"val_acc\": val_acc\n",
    "            }, best_ckpt_path)\n",
    "            print(\"Saved best fine-tune checkpoint:\", best_ckpt_path)\n",
    "\n",
    "        # Periodic resume checkpoint\n",
    "        if save_every_epoch:\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": ft_model.state_dict(),\n",
    "                \"optimizer_state\": ft_optimizer.state_dict(),\n",
    "                \"scheduler_state\": ft_scheduler.state_dict(),\n",
    "                \"val_acc\": val_acc\n",
    "            }, resume_ckpt_path)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt caught — saving resume checkpoint...\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": ft_model.state_dict(),\n",
    "        \"optimizer_state\": ft_optimizer.state_dict(),\n",
    "        \"scheduler_state\": ft_scheduler.state_dict(),\n",
    "        \"val_acc\": best_val_acc\n",
    "    }, resume_ckpt_path)\n",
    "    print(\"Saved resume checkpoint to\", resume_ckpt_path)\n",
    "    raise\n",
    "\n",
    "print(\"Fine-tune complete. Best val acc:\", best_val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 4.3) CELL 13 - Embedding visualization: UMAP, t-SNE, PCA & silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T03:30:45.794826Z",
     "iopub.status.busy": "2025-12-01T03:30:45.794219Z",
     "iopub.status.idle": "2025-12-01T03:30:54.394676Z",
     "shell.execute_reply": "2025-12-01T03:30:54.393975Z",
     "shell.execute_reply.started": "2025-12-01T03:30:45.794791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/simsiam_task4/pca_proj.png\n",
      "UMAP failed: TypeError(\"check_array() got an unexpected keyword argument 'ensure_all_finite'\")\n",
      "Saved: /kaggle/working/simsiam_task4/umap_proj_fallback_pca.png\n",
      "Saved PCA proxy as UMAP fallback.\n",
      "Saved: /kaggle/working/simsiam_task4/tsne_proj.png\n",
      "Silhouette score (features): -0.019335013\n",
      "Embedding viz cell finished. Check files in: /kaggle/working/simsiam_task4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "except Exception:\n",
    "    TSNE = None\n",
    "\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "\n",
    "OUT_DIR = globals().get(\"OUT_DIR\", \"/kaggle/working/simsiam_task4\")\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def safe_load(npname):\n",
    "    p = os.path.join(OUT_DIR, npname)\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "    return np.load(p, allow_pickle=False)\n",
    "\n",
    "train_feats = safe_load(\"train_feats.npy\")\n",
    "val_feats   = safe_load(\"val_feats.npy\")\n",
    "test_feats  = safe_load(\"test_feats.npy\")\n",
    "train_lbls  = safe_load(\"train_labels.npy\")\n",
    "val_lbls    = safe_load(\"val_labels.npy\")\n",
    "test_lbls   = safe_load(\"test_labels.npy\")\n",
    "\n",
    "feats_all = np.vstack([train_feats, val_feats, test_feats]).astype(np.float32)\n",
    "lbls_all  = np.concatenate([train_lbls, val_lbls, test_lbls]).astype(int)\n",
    "\n",
    "classes = globals().get(\"classes\", [\"Diseased\", \"Dried\", \"Healthy\"])\n",
    "SEED = globals().get(\"SEED\", 42)\n",
    "\n",
    "\n",
    "def scatter_and_save(X2, labels, title, fname):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for c_idx, c_name in enumerate(classes):\n",
    "        idx = labels == c_idx\n",
    "        if np.any(idx):\n",
    "            plt.scatter(X2[idx,0], X2[idx,1], label=c_name, s=10, alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    outpath = os.path.join(OUT_DIR, fname)\n",
    "    plt.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", outpath)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "proj_pca = pca.fit_transform(feats_all)\n",
    "scatter_and_save(proj_pca, lbls_all, \"PCA projection of features\", \"pca_proj.png\")\n",
    "\n",
    "\n",
    "if umap is None:\n",
    "    print(\"UMAP not installed or import failed; skipping UMAP projection.\")\n",
    "else:\n",
    "    try:\n",
    "        \n",
    "        reducer = umap.UMAP(n_components=2, random_state=SEED)\n",
    "        proj_umap = reducer.fit_transform(feats_all)\n",
    "        scatter_and_save(proj_umap, lbls_all, \"UMAP projection of features\", \"umap_proj.png\")\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(\"UMAP failed:\", repr(e))\n",
    "        \n",
    "        scatter_and_save(proj_pca, lbls_all, \"UMAP fallback (PCA proxy)\", \"umap_proj_fallback_pca.png\")\n",
    "        print(\"Saved PCA proxy as UMAP fallback.\")\n",
    "\n",
    "\n",
    "if TSNE is None:\n",
    "    print(\"TSNE not available; skipping t-SNE projection.\")\n",
    "else:\n",
    "    try:\n",
    "        n_samples = feats_all.shape[0]\n",
    "        tsne_max = 2000\n",
    "        if n_samples > tsne_max:\n",
    "            idx_keep = []\n",
    "            rng = np.random.RandomState(SEED)\n",
    "            per_class = max(50, int(tsne_max / max(1, len(classes))))\n",
    "            lbls_arr = lbls_all\n",
    "            for c in range(len(classes)):\n",
    "                positions = np.where(lbls_arr == c)[0]\n",
    "                if len(positions) == 0:\n",
    "                    continue\n",
    "                k = min(len(positions), per_class)\n",
    "                sel = rng.choice(positions, size=k, replace=False)\n",
    "                idx_keep.extend(sel.tolist())\n",
    "            idx_keep = np.array(sorted(idx_keep))\n",
    "            feats_tsne = feats_all[idx_keep]\n",
    "            labels_tsne = lbls_all[idx_keep]\n",
    "            print(f\"t-SNE: dataset too large ({n_samples}), subsampling to {len(idx_keep)} samples for speed.\")\n",
    "        else:\n",
    "            feats_tsne = feats_all\n",
    "            labels_tsne = lbls_all\n",
    "\n",
    "        tsne = TSNE(n_components=2, perplexity=30, random_state=SEED, init='pca')\n",
    "        proj_tsne = tsne.fit_transform(feats_tsne)\n",
    "        \n",
    "        fname = \"tsne_proj.png\" if feats_tsne.shape[0] == n_samples else \"tsne_proj_subsample.png\"\n",
    "        scatter_and_save(proj_tsne, labels_tsne, \"t-SNE projection of features\", fname)\n",
    "    except Exception as e:\n",
    "        print(\"t-SNE failed or was interrupted:\", repr(e))\n",
    "        print(\"Skipping t-SNE.\")\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "        unique, counts = np.unique(lbls_all, return_counts=True)\n",
    "    if len(unique) < 2 or np.min(counts) < 2:\n",
    "        print(\"Silhouette not computed: need >=2 classes and >=2 samples per class.\")\n",
    "        sil = None\n",
    "    else:\n",
    "        sil = silhouette_score(feats_all, lbls_all)\n",
    "        print(\"Silhouette score (features):\", sil)\n",
    "        with open(os.path.join(OUT_DIR, \"embedding_stats.txt\"), \"w\") as f:\n",
    "            f.write(f\"Silhouette: {sil}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Silhouette computation failed:\", repr(e))\n",
    "    sil = None\n",
    "\n",
    "print(\"Embedding viz cell finished. Check files in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 4.4) CELL 14 - Label-efficiency experiments (1%,5%,10%,25%,50% labeled) using linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T03:30:57.659173Z",
     "iopub.status.busy": "2025-12-01T03:30:57.658886Z",
     "iopub.status.idle": "2025-12-01T03:31:08.679058Z",
     "shell.execute_reply": "2025-12-01T03:31:08.678345Z",
     "shell.execute_reply.started": "2025-12-01T03:30:57.659153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction 1% -> Test Acc: 0.6250\n",
      "Fraction 5% -> Test Acc: 0.7472\n",
      "Fraction 10% -> Test Acc: 0.7778\n",
      "Fraction 25% -> Test Acc: 0.7889\n",
      "Fraction 50% -> Test Acc: 0.8167\n",
      "Fraction 100% -> Test Acc: 0.8167\n",
      "Saved label-efficiency results to /kaggle/working/simsiam_task4/label_efficiency.json\n"
     ]
    }
   ],
   "source": [
    "fractions = [0.01, 0.05, 0.10, 0.25, 0.50, 1.0]\n",
    "results = {}\n",
    "total_train = len(train_feats)\n",
    "for frac in fractions:\n",
    "    n = max(1, int(total_train * frac))\n",
    "\n",
    "    subs_idx = []\n",
    "    train_lbls_arr = np.array(train_lbls)\n",
    "    for c in range(len(classes)):\n",
    "        idxs = np.where(train_lbls_arr == c)[0]\n",
    "        k = max(1, int(len(idxs) * frac))\n",
    "        rng = np.random.RandomState(SEED)\n",
    "        sel = rng.choice(idxs, size=k, replace=False)\n",
    "        subs_idx.extend(sel.tolist())\n",
    "    subs_idx = sorted(subs_idx)\n",
    "    X_sub = train_feats[subs_idx]\n",
    "    y_sub = train_lbls_arr[subs_idx]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    clf.fit(X_sub, y_sub)\n",
    "    test_pred = clf.predict(test_feats)\n",
    "    acc = accuracy_score(test_lbls, test_pred)\n",
    "    results[f\"{int(frac*100)}%\"] = float(acc)\n",
    "    print(f\"Fraction {int(frac*100)}% -> Test Acc: {acc:.4f}\")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"label_efficiency.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved label-efficiency results to\", os.path.join(OUT_DIR, \"label_efficiency.json\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8515242,
     "sourceId": 13416578,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
