{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13547290,"sourceType":"datasetVersion","datasetId":8603845},{"sourceId":14165553,"sourceType":"datasetVersion","datasetId":9029279},{"sourceId":14171689,"sourceType":"datasetVersion","datasetId":9033236},{"sourceId":14172362,"sourceType":"datasetVersion","datasetId":9033708}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ac7615d6","cell_type":"markdown","source":"# Task 7 — Ratios, Ablations, and Statistics (SimCLR + SimSiam)\n\nThis notebook consolidates **downstream evaluations**, **ablations**, and **statistical tests** for the two SSL notebooks you have:\n\n- **SimCLR** (from `ssl-1 (1).ipynb`)\n- **SimSiam** (from `Task-(5+6) - Simsiam SSL Part ... .ipynb`)\n\n✅ Deliverables included here:\n- Ablation tables (linear-probe + label-efficiency)\n- Statistical tests (paired t-test; Friedman + Nemenyi post-hoc)\n- 1–2 effect-size figures (Cohen’s d + mean differences)\n\n> **How to use:** run cells top-to-bottom.  \n> If you already have trained checkpoints, set `USE_PRETRAINED=True` and point paths to your `.pth` encoders.\n","metadata":{}},{"id":"f04f2375","cell_type":"code","source":"# ==== Install/Imports ====\nimport os, random, math\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom scipy import stats\nfrom statsmodels.stats.libqsturng import psturng\n\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"device:\", device)\n\ndef seed_all(seed: int = 42):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nseed_all(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:28:21.842885Z","iopub.execute_input":"2025-12-16T06:28:21.843836Z","iopub.status.idle":"2025-12-16T06:28:21.851617Z","shell.execute_reply.started":"2025-12-16T06:28:21.843801Z","shell.execute_reply":"2025-12-16T06:28:21.850876Z"}},"outputs":[{"name":"stdout","text":"device: cuda\n","output_type":"stream"}],"execution_count":41},{"id":"a2318015","cell_type":"markdown","source":"## 1) Common dataset loader\n\nBoth of your original notebooks use **ImageFolder-style** datasets (folder per class).  \nSet `DATA_DIR` to your dataset root (the folder that contains class subfolders).","metadata":{}},{"id":"2ff25e14","cell_type":"code","source":"# ==== Set your dataset root ====\nDATA_DIR = \"/kaggle/input/betel/Controlled Environment\"\nassert os.path.exists(DATA_DIR), f\"DATA_DIR not found: {DATA_DIR}\"\nprint(\"DATA_DIR OK:\", DATA_DIR)\n\n# Basic eval transform (same for all downstream tasks)\neval_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n])\n\nfull_ds = ImageFolder(DATA_DIR, transform=eval_transform)\nX = np.arange(len(full_ds))\ny = np.array([full_ds[i][1] for i in range(len(full_ds))])\nnum_classes = len(full_ds.classes)\n\nprint(\"N:\", len(full_ds), \"classes:\", num_classes, full_ds.classes[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:28:26.797942Z","iopub.execute_input":"2025-12-16T06:28:26.798621Z","iopub.status.idle":"2025-12-16T06:35:34.395513Z","shell.execute_reply.started":"2025-12-16T06:28:26.798599Z","shell.execute_reply":"2025-12-16T06:35:34.394807Z"}},"outputs":[{"name":"stdout","text":"DATA_DIR OK: /kaggle/input/betel/Controlled Environment\nN: 893 classes: 3 ['Diseased', 'Dried', 'Healthy']\n","output_type":"stream"}],"execution_count":42},{"id":"a0d1bd32","cell_type":"markdown","source":"## 2) Train:test grid (same idea as Task 2)\n\nIf your Task 2 used a specific grid, put it in `TRAIN_TEST_GRID` below.  \nDefault: **(90:10), (80:20), (70:30)**.\n\nWe evaluate each setting using **Stratified K-Fold** on the *train portion* and keep test held out.\n","metadata":{}},{"id":"7805b0e7","cell_type":"code","source":"# ==== Train:test grid ====\nTRAIN_TEST_GRID = [\n    {\"name\": \"90_10\", \"test_size\": 0.10},\n    {\"name\": \"80_20\", \"test_size\": 0.20},\n    {\"name\": \"70_30\", \"test_size\": 0.30},\n]\n\nN_FOLDS = 5\nSEED = 42\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:35:49.390254Z","iopub.execute_input":"2025-12-16T06:35:49.390992Z","iopub.status.idle":"2025-12-16T06:35:49.394746Z","shell.execute_reply.started":"2025-12-16T06:35:49.390965Z","shell.execute_reply":"2025-12-16T06:35:49.393888Z"}},"outputs":[],"execution_count":49},{"id":"fe827148","cell_type":"code","source":"def make_splits(X, y, test_size: float, n_folds: int, seed: int):\n    # 1) Hold-out test split\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n    train_idx, test_idx = next(sss.split(X, y))\n    X_train, y_train = X[train_idx], y[train_idx]\n\n    # 2) K-fold on train split (for paired stats across identical folds)\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    folds = []\n    for tr, va in skf.split(X_train, y_train):\n        folds.append({\n            \"train_idx\": train_idx[tr],\n            \"val_idx\": train_idx[va],\n            \"test_idx\": test_idx\n        })\n    return folds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:35:58.177797Z","iopub.execute_input":"2025-12-16T06:35:58.178140Z","iopub.status.idle":"2025-12-16T06:35:58.183380Z","shell.execute_reply.started":"2025-12-16T06:35:58.178100Z","shell.execute_reply":"2025-12-16T06:35:58.182813Z"}},"outputs":[],"execution_count":56},{"id":"586ea75a","cell_type":"markdown","source":"## 3) Metrics helpers","metadata":{}},{"id":"b76c3841","cell_type":"code","source":"def compute_metrics(y_true, y_pred):\n    return {\n        \"acc\": float(accuracy_score(y_true, y_pred)),\n        \"f1_macro\": float(f1_score(y_true, y_pred, average=\"macro\")),\n    }\n\n@torch.no_grad()\ndef extract_features(encoder: nn.Module, loader: DataLoader):\n    encoder.eval()\n    feats, labels = [], []\n    for xb, yb in loader:\n        xb = xb.to(device)\n        z = encoder(xb)\n        if isinstance(z, (tuple, list)):  # safety\n            z = z[0]\n        feats.append(z.detach().cpu().numpy())\n        labels.append(yb.numpy())\n    return np.concatenate(feats, axis=0), np.concatenate(labels, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:03.354516Z","iopub.execute_input":"2025-12-16T06:36:03.354788Z","iopub.status.idle":"2025-12-16T06:36:03.360209Z","shell.execute_reply.started":"2025-12-16T06:36:03.354738Z","shell.execute_reply":"2025-12-16T06:36:03.359514Z"}},"outputs":[],"execution_count":59},{"id":"bc41d51e","cell_type":"markdown","source":"## 4) Downstream evaluators\n\nWe do:\n- **Linear probe**: freeze encoder, train a single linear layer on train fold, validate on val fold, report on test.\n- **Label efficiency**: train linear probe using only `{1,5,10,25,50}%` of labeled train-fold data, evaluate on test.\n\n> Tip: keep epochs small for quick runs, then increase for final reporting.\n","metadata":{}},{"id":"db927c97","cell_type":"code","source":"class LinearProbe(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, num_classes)\n    def forward(self, x): return self.fc(x)\n\ndef fit_linear_probe(train_feats, train_y, val_feats, val_y, num_classes, \n                     lr=1e-3, wd=1e-4, epochs=50, batch_size=256, seed=42):\n    seed_all(seed)\n    in_dim = train_feats.shape[1]\n    model = LinearProbe(in_dim, num_classes).to(device)\n\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n    loss_fn = nn.CrossEntropyLoss()\n\n    # Torch tensors\n    Xtr = torch.tensor(train_feats, dtype=torch.float32)\n    ytr = torch.tensor(train_y, dtype=torch.long)\n    Xva = torch.tensor(val_feats, dtype=torch.float32)\n    yva = torch.tensor(val_y, dtype=torch.long)\n\n    best_state, best_acc = None, -1.0\n    for ep in range(epochs):\n        model.train()\n        idx = torch.randperm(len(Xtr))\n        for i in range(0, len(Xtr), batch_size):\n            b = idx[i:i+batch_size]\n            xb = Xtr[b].to(device); yb = ytr[b].to(device)\n            opt.zero_grad()\n            logits = model(xb)\n            loss = loss_fn(logits, yb)\n            loss.backward()\n            opt.step()\n\n        # quick val\n        model.eval()\n        pred = model(Xva.to(device)).argmax(1).detach().cpu().numpy()\n        acc = accuracy_score(val_y, pred)\n        if acc > best_acc:\n            best_acc = acc\n            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    return model\n\ndef evaluate_linear_probe_for_split(encoder, full_ds, split, num_classes,\n                                   feat_bs=128, lp_epochs=50, seed=42):\n    # loaders\n    tr = DataLoader(Subset(full_ds, split[\"train_idx\"]), batch_size=feat_bs, shuffle=False, num_workers=0)\n    va = DataLoader(Subset(full_ds, split[\"val_idx\"]),   batch_size=feat_bs, shuffle=False, num_workers=0)\n    te = DataLoader(Subset(full_ds, split[\"test_idx\"]),  batch_size=feat_bs, shuffle=False, num_workers=0)\n\n    trF, trY = extract_features(encoder, tr)\n    vaF, vaY = extract_features(encoder, va)\n    teF, teY = extract_features(encoder, te)\n\n    lp = fit_linear_probe(trF, trY, vaF, vaY, num_classes=num_classes, epochs=lp_epochs, seed=seed)\n    te_pred = lp(torch.tensor(teF, dtype=torch.float32).to(device)).argmax(1).detach().cpu().numpy()\n    return compute_metrics(teY, te_pred)\n\ndef label_efficiency_eval(encoder, full_ds, split, num_classes, fractions=(0.01,0.05,0.10,0.25,0.50),\n                          feat_bs=128, lp_epochs=50, seed=42):\n    tr = DataLoader(Subset(full_ds, split[\"train_idx\"]), batch_size=feat_bs, shuffle=False, num_workers=0)\n    te = DataLoader(Subset(full_ds, split[\"test_idx\"]),  batch_size=feat_bs, shuffle=False, num_workers=0)\n    trF, trY = extract_features(encoder, tr)\n    teF, teY = extract_features(encoder, te)\n\n    out=[]\n    rng = np.random.default_rng(seed)\n    for frac in fractions:\n        # stratified subsample of train fold\n        idx = np.arange(len(trY))\n        # per-class sampling\n        chosen=[]\n        for c in np.unique(trY):\n            c_idx = idx[trY==c]\n            k = max(1, int(round(len(c_idx)*frac)))\n            chosen.append(rng.choice(c_idx, size=k, replace=False))\n        chosen = np.concatenate(chosen)\n        rng.shuffle(chosen)\n\n        subF, subY = trF[chosen], trY[chosen]\n\n        # use small val split from subsample (or reuse itself if tiny)\n        if len(subY) >= 20:\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n            tr_i, va_i = next(sss.split(np.arange(len(subY)), subY))\n            lp = fit_linear_probe(subF[tr_i], subY[tr_i], subF[va_i], subY[va_i], num_classes=num_classes, epochs=lp_epochs, seed=seed)\n        else:\n            lp = fit_linear_probe(subF, subY, subF, subY, num_classes=num_classes, epochs=max(10, lp_epochs//2), seed=seed)\n\n        pred = lp(torch.tensor(teF, dtype=torch.float32).to(device)).argmax(1).detach().cpu().numpy()\n        m = compute_metrics(teY, pred)\n        m[\"frac\"] = frac\n        out.append(m)\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:36:50.724860Z","iopub.execute_input":"2025-12-16T06:36:50.725530Z","iopub.status.idle":"2025-12-16T06:36:50.739727Z","shell.execute_reply.started":"2025-12-16T06:36:50.725506Z","shell.execute_reply":"2025-12-16T06:36:50.738937Z"}},"outputs":[],"execution_count":64},{"id":"f80b915a","cell_type":"markdown","source":"## 5) Load encoders (baseline + ablation)\n\nYou have two SSLs:\n- **SimCLR** notebook produces an encoder + projection head (for pretraining). For downstream we use the **encoder output**.\n- **SimSiam** notebook produces an encoder (plus projector/predictor). For downstream we use the **encoder output**.\n\nBelow we provide **two options**:\n\n### Option A: Use your existing trained checkpoints (recommended)\nSet paths to your `.pth` files.\n\n### Option B: Re-run pretraining in this notebook\nPaste your training loops from each notebook and return the trained encoder.\n","metadata":{}},{"id":"6d670052","cell_type":"code","source":"# ==== Option A (recommended): Load pretrained encoders ====\nUSE_PRETRAINED = True\n\nSIMCLR_ENCODER_CKPT = \"/kaggle/input/simclr-encoder-frozen-path/simclr_encoder_frozen.pth\"     # change if needed\nSIMSIAM_ENCODER_CKPT = \"/kaggle/input/simsiam-encoder-pth/simsiam_encoder.pth\"          # change if needed\nBYOL_ENCODER_CKPT = \"/kaggle/input/byol-encodder-pth/byol_resnet18_encoder.pth\"\n# Simple ResNet-18 encoder builder (matches typical code; edit if your original differs)\nfrom torchvision.models import resnet18\n\ndef build_resnet18_encoder(out_dim=512):\n    m = resnet18(weights=None)\n    m.fc = nn.Identity()\n    return m\n\ndef load_encoder(ckpt_path: str):\n    enc = build_resnet18_encoder().to(device)\n    sd = torch.load(ckpt_path, map_location=\"cpu\")\n    # If checkpoint contains extra keys, you may need to adapt this.\n    enc.load_state_dict(sd, strict=False)\n    return enc\n\nencoders = {}\n\nif USE_PRETRAINED:\n    assert os.path.exists(SIMCLR_ENCODER_CKPT), f\"Missing: {SIMCLR_ENCODER_CKPT}\"\n    assert os.path.exists(SIMSIAM_ENCODER_CKPT), f\"Missing: {SIMSIAM_ENCODER_CKPT}\"\n    assert os.path.exists(BYOL_ENCODER_CKPT), f\"Missing: {BYOL_ENCODER_CKPT}\"\n    encoders[\"SimCLR_base\"] = load_encoder(SIMCLR_ENCODER_CKPT)\n    encoders[\"SimSiam_base\"] = load_encoder(SIMSIAM_ENCODER_CKPT)\n    encoders[\"Byol_base\"] = load_encoder(BYOL_ENCODER_CKPT)\n    print(\"Loaded encoders:\", list(encoders.keys()))\nelse:\n    print(\"Set USE_PRETRAINED=True or paste pretraining code in the next cell.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:39:52.481173Z","iopub.execute_input":"2025-12-16T06:39:52.481587Z","iopub.status.idle":"2025-12-16T06:39:53.800307Z","shell.execute_reply.started":"2025-12-16T06:39:52.481563Z","shell.execute_reply":"2025-12-16T06:39:53.799603Z"}},"outputs":[{"name":"stdout","text":"Loaded encoders: ['SimCLR_base', 'SimSiam_base', 'Byol_base']\n","output_type":"stream"}],"execution_count":65},{"id":"84f189f8","cell_type":"code","source":"# ==== Option B: Paste your ablation encoders here ====\n# You must create encoders dict entries like:\n# encoders[\"SimCLR_abl\"] = <trained encoder model>\n# encoders[\"SimSiam_abl\"] = <trained encoder model>\n\n# Example ablations (what you SHOULD implement):\n# - SimCLR ablation: remove ColorJitter OR reduce projection head depth (2-layer -> 1-layer)\n# - SimSiam ablation: remove a key augmentation (e.g., no vertical flip) OR change predictor MLP depth\n\n# After training, put checkpoints in OUT_DIR and load them above.\n\npass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:40:06.159160Z","iopub.execute_input":"2025-12-16T06:40:06.159406Z","iopub.status.idle":"2025-12-16T06:40:06.163380Z","shell.execute_reply.started":"2025-12-16T06:40:06.159391Z","shell.execute_reply":"2025-12-16T06:40:06.162639Z"}},"outputs":[],"execution_count":67},{"id":"bbcff642","cell_type":"markdown","source":"## 6) Define ablation configs (at least one per SSL)\n\nEven if you load checkpoints, fill this table so your report clearly states what changed.\n","metadata":{}},{"id":"f1cd6790","cell_type":"code","source":"ablations = pd.DataFrame([\n    {\n        \"ssl\": \"SimCLR\",\n        \"variant\": \"base\",\n        \"change\": \"Original augmentations + original projection head + original epochs/batch/LR\",\n    },\n    {\n        \"ssl\": \"SimCLR\",\n        \"variant\": \"abl\",\n        \"change\": \"ABLATION EXAMPLE: remove ColorJitter (or reduce projection head depth 2→1)\",\n    },\n    {\n        \"ssl\": \"SimSiam\",\n        \"variant\": \"base\",\n        \"change\": \"Original two-view transform + projector+predictor + original epochs/batch/LR\",\n    },\n    {\n        \"ssl\": \"SimSiam\",\n        \"variant\": \"abl\",\n        \"change\": \"ABLATION EXAMPLE: remove VerticalFlip (or reduce predictor depth / change optimizer)\",\n    },\n    {\n        \"ssl\": \"Byol\",\n        \"variant\": \"base\",\n        \"change\": \"Original two-view transform + projector+predictor + original epochs/batch/LR\",\n    },\n    {\n        \"ssl\": \"Byol\",\n        \"variant\": \"abl\",\n        \"change\": \"ABLATION EXAMPLE: remove VerticalFlip (or reduce predictor depth / change optimizer)\",\n    },\n])\nablations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:43:29.727958Z","iopub.execute_input":"2025-12-16T06:43:29.728633Z","iopub.status.idle":"2025-12-16T06:43:29.737538Z","shell.execute_reply.started":"2025-12-16T06:43:29.728610Z","shell.execute_reply":"2025-12-16T06:43:29.736739Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"       ssl variant                                             change\n0   SimCLR    base  Original augmentations + original projection h...\n1   SimCLR     abl  ABLATION EXAMPLE: remove ColorJitter (or reduc...\n2  SimSiam    base  Original two-view transform + projector+predic...\n3  SimSiam     abl  ABLATION EXAMPLE: remove VerticalFlip (or redu...\n4     Byol    base  Original two-view transform + projector+predic...\n5     Byol     abl  ABLATION EXAMPLE: remove VerticalFlip (or redu...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ssl</th>\n      <th>variant</th>\n      <th>change</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SimCLR</td>\n      <td>base</td>\n      <td>Original augmentations + original projection h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SimCLR</td>\n      <td>abl</td>\n      <td>ABLATION EXAMPLE: remove ColorJitter (or reduc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SimSiam</td>\n      <td>base</td>\n      <td>Original two-view transform + projector+predic...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SimSiam</td>\n      <td>abl</td>\n      <td>ABLATION EXAMPLE: remove VerticalFlip (or redu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Byol</td>\n      <td>base</td>\n      <td>Original two-view transform + projector+predic...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Byol</td>\n      <td>abl</td>\n      <td>ABLATION EXAMPLE: remove VerticalFlip (or redu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":79},{"id":"49879f51","cell_type":"markdown","source":"## 7) Run evaluations across the train:test grid\n\nOutputs:\n- `results_lp`: linear probe results per fold (for stats)\n- `results_le`: label-efficiency results per fold and fraction\n","metadata":{}},{"id":"59155539","cell_type":"code","source":"# ==== Evaluation knobs (FAST) ====\nLP_EPOCHS = 10          # আগে কম রাখো, ফাইনালে 50 করো\nFEAT_BS = 256           # বড় batch = দ্রুত\nLABEL_FRACTIONS = (0.10, 0.50)   # আগে কম fraction, ফাইনালে (0.01,0.05,0.10,0.25,0.50)\n\nresults_lp = []\nresults_le = []\n\n# ---- Feature cache to avoid repeated extraction ----\nfeat_cache = {}  # key: (model, grid, fold, split_name) -> (F, Y)\n\ndef get_feats(enc, ds, idxs, key, bs=256):\n    if key in feat_cache:\n        return feat_cache[key]\n    loader = DataLoader(\n        Subset(ds, idxs),\n        batch_size=bs,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    F, Y = extract_features(enc, loader)\n    feat_cache[key] = (F, Y)\n    return F, Y\n\nfor grid in TRAIN_TEST_GRID:\n    folds = make_splits(X, y, test_size=grid[\"test_size\"], n_folds=N_FOLDS, seed=SEED)\n\n    for fold_id, split in enumerate(folds):\n        for model_name, enc in encoders.items():\n            print(f\"[{grid['name']}] fold={fold_id} model={model_name} -> extracting feats...\", flush=True)\n\n            # ---- Extract features ONCE per split ----\n            trF, trY = get_feats(enc, full_ds, split[\"train_idx\"], (model_name, grid[\"name\"], fold_id, \"train\"), bs=FEAT_BS)\n            vaF, vaY = get_feats(enc, full_ds, split[\"val_idx\"],   (model_name, grid[\"name\"], fold_id, \"val\"),   bs=FEAT_BS)\n            teF, teY = get_feats(enc, full_ds, split[\"test_idx\"],  (model_name, grid[\"name\"], fold_id, \"test\"),  bs=FEAT_BS)\n\n            # ---- Linear probe ----\n            print(f\"[{grid['name']}] fold={fold_id} model={model_name} -> linear probe...\", flush=True)\n            lp = fit_linear_probe(trF, trY, vaF, vaY, num_classes=num_classes, epochs=LP_EPOCHS, seed=SEED+fold_id)\n\n            with torch.no_grad():\n                te_pred = lp(torch.tensor(teF, dtype=torch.float32).to(device)).argmax(1).cpu().numpy()\n\n            m = compute_metrics(teY, te_pred)\n            results_lp.append({\n                \"grid\": grid[\"name\"],\n                \"test_size\": grid[\"test_size\"],\n                \"fold\": fold_id,\n                \"model\": model_name,\n                **m\n            })\n\n            # ---- Label efficiency (reuse SAME extracted trF/teF) ----\n            print(f\"[{grid['name']}] fold={fold_id} model={model_name} -> label efficiency...\", flush=True)\n\n            rng = np.random.default_rng(SEED + fold_id)\n            idx_all = np.arange(len(trY))\n\n            for frac in LABEL_FRACTIONS:\n                chosen = []\n                for c in np.unique(trY):\n                    c_idx = idx_all[trY == c]\n                    k = max(1, int(round(len(c_idx) * frac)))\n                    chosen.append(rng.choice(c_idx, size=k, replace=False))\n                chosen = np.concatenate(chosen)\n                rng.shuffle(chosen)\n\n                subF, subY = trF[chosen], trY[chosen]\n\n                # small val split inside subsample\n                if len(subY) >= 20:\n                    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED+fold_id)\n                    tr_i, va_i = next(sss.split(np.arange(len(subY)), subY))\n                    lp2 = fit_linear_probe(subF[tr_i], subY[tr_i], subF[va_i], subY[va_i],\n                                           num_classes=num_classes, epochs=LP_EPOCHS, seed=SEED+fold_id)\n                else:\n                    lp2 = fit_linear_probe(subF, subY, subF, subY,\n                                           num_classes=num_classes, epochs=max(5, LP_EPOCHS//2), seed=SEED+fold_id)\n\n                with torch.no_grad():\n                    pred = lp2(torch.tensor(teF, dtype=torch.float32).to(device)).argmax(1).cpu().numpy()\n                mm = compute_metrics(teY, pred)\n                mm[\"frac\"] = frac\n\n                results_le.append({\n                    \"grid\": grid[\"name\"],\n                    \"test_size\": grid[\"test_size\"],\n                    \"fold\": fold_id,\n                    \"model\": model_name,\n                    **mm\n                })\n\n            print(f\"[{grid['name']}] fold={fold_id} model={model_name} DONE ✅\", flush=True)\n\nresults_lp = pd.DataFrame(results_lp)\nresults_le = pd.DataFrame(results_le)\n\ndisplay(results_lp.head())\ndisplay(results_le.head())\nprint(\"LP rows:\", len(results_lp), \"LE rows:\", len(results_le))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:43:49.899144Z","iopub.execute_input":"2025-12-16T06:43:49.899420Z","iopub.status.idle":"2025-12-16T10:35:03.249492Z","shell.execute_reply.started":"2025-12-16T06:43:49.899400Z","shell.execute_reply":"2025-12-16T10:35:03.248811Z"}},"outputs":[{"name":"stdout","text":"[90_10] fold=0 model=SimCLR_base -> extracting feats...\n[90_10] fold=0 model=SimCLR_base -> linear probe...\n[90_10] fold=0 model=SimCLR_base -> label efficiency...\n[90_10] fold=0 model=SimCLR_base DONE ✅\n[90_10] fold=0 model=SimSiam_base -> extracting feats...\n[90_10] fold=0 model=SimSiam_base -> linear probe...\n[90_10] fold=0 model=SimSiam_base -> label efficiency...\n[90_10] fold=0 model=SimSiam_base DONE ✅\n[90_10] fold=0 model=Byol_base -> extracting feats...\n[90_10] fold=0 model=Byol_base -> linear probe...\n[90_10] fold=0 model=Byol_base -> label efficiency...\n[90_10] fold=0 model=Byol_base DONE ✅\n[90_10] fold=1 model=SimCLR_base -> extracting feats...\n[90_10] fold=1 model=SimCLR_base -> linear probe...\n[90_10] fold=1 model=SimCLR_base -> label efficiency...\n[90_10] fold=1 model=SimCLR_base DONE ✅\n[90_10] fold=1 model=SimSiam_base -> extracting feats...\n[90_10] fold=1 model=SimSiam_base -> linear probe...\n[90_10] fold=1 model=SimSiam_base -> label efficiency...\n[90_10] fold=1 model=SimSiam_base DONE ✅\n[90_10] fold=1 model=Byol_base -> extracting feats...\n[90_10] fold=1 model=Byol_base -> linear probe...\n[90_10] fold=1 model=Byol_base -> label efficiency...\n[90_10] fold=1 model=Byol_base DONE ✅\n[90_10] fold=2 model=SimCLR_base -> extracting feats...\n[90_10] fold=2 model=SimCLR_base -> linear probe...\n[90_10] fold=2 model=SimCLR_base -> label efficiency...\n[90_10] fold=2 model=SimCLR_base DONE ✅\n[90_10] fold=2 model=SimSiam_base -> extracting feats...\n[90_10] fold=2 model=SimSiam_base -> linear probe...\n[90_10] fold=2 model=SimSiam_base -> label efficiency...\n[90_10] fold=2 model=SimSiam_base DONE ✅\n[90_10] fold=2 model=Byol_base -> extracting feats...\n[90_10] fold=2 model=Byol_base -> linear probe...\n[90_10] fold=2 model=Byol_base -> label efficiency...\n[90_10] fold=2 model=Byol_base DONE ✅\n[90_10] fold=3 model=SimCLR_base -> extracting feats...\n[90_10] fold=3 model=SimCLR_base -> linear probe...\n[90_10] fold=3 model=SimCLR_base -> label efficiency...\n[90_10] fold=3 model=SimCLR_base DONE ✅\n[90_10] fold=3 model=SimSiam_base -> extracting feats...\n[90_10] fold=3 model=SimSiam_base -> linear probe...\n[90_10] fold=3 model=SimSiam_base -> label efficiency...\n[90_10] fold=3 model=SimSiam_base DONE ✅\n[90_10] fold=3 model=Byol_base -> extracting feats...\n[90_10] fold=3 model=Byol_base -> linear probe...\n[90_10] fold=3 model=Byol_base -> label efficiency...\n[90_10] fold=3 model=Byol_base DONE ✅\n[90_10] fold=4 model=SimCLR_base -> extracting feats...\n[90_10] fold=4 model=SimCLR_base -> linear probe...\n[90_10] fold=4 model=SimCLR_base -> label efficiency...\n[90_10] fold=4 model=SimCLR_base DONE ✅\n[90_10] fold=4 model=SimSiam_base -> extracting feats...\n[90_10] fold=4 model=SimSiam_base -> linear probe...\n[90_10] fold=4 model=SimSiam_base -> label efficiency...\n[90_10] fold=4 model=SimSiam_base DONE ✅\n[90_10] fold=4 model=Byol_base -> extracting feats...\n[90_10] fold=4 model=Byol_base -> linear probe...\n[90_10] fold=4 model=Byol_base -> label efficiency...\n[90_10] fold=4 model=Byol_base DONE ✅\n[80_20] fold=0 model=SimCLR_base -> extracting feats...\n[80_20] fold=0 model=SimCLR_base -> linear probe...\n[80_20] fold=0 model=SimCLR_base -> label efficiency...\n[80_20] fold=0 model=SimCLR_base DONE ✅\n[80_20] fold=0 model=SimSiam_base -> extracting feats...\n[80_20] fold=0 model=SimSiam_base -> linear probe...\n[80_20] fold=0 model=SimSiam_base -> label efficiency...\n[80_20] fold=0 model=SimSiam_base DONE ✅\n[80_20] fold=0 model=Byol_base -> extracting feats...\n[80_20] fold=0 model=Byol_base -> linear probe...\n[80_20] fold=0 model=Byol_base -> label efficiency...\n[80_20] fold=0 model=Byol_base DONE ✅\n[80_20] fold=1 model=SimCLR_base -> extracting feats...\n[80_20] fold=1 model=SimCLR_base -> linear probe...\n[80_20] fold=1 model=SimCLR_base -> label efficiency...\n[80_20] fold=1 model=SimCLR_base DONE ✅\n[80_20] fold=1 model=SimSiam_base -> extracting feats...\n[80_20] fold=1 model=SimSiam_base -> linear probe...\n[80_20] fold=1 model=SimSiam_base -> label efficiency...\n[80_20] fold=1 model=SimSiam_base DONE ✅\n[80_20] fold=1 model=Byol_base -> extracting feats...\n[80_20] fold=1 model=Byol_base -> linear probe...\n[80_20] fold=1 model=Byol_base -> label efficiency...\n[80_20] fold=1 model=Byol_base DONE ✅\n[80_20] fold=2 model=SimCLR_base -> extracting feats...\n[80_20] fold=2 model=SimCLR_base -> linear probe...\n[80_20] fold=2 model=SimCLR_base -> label efficiency...\n[80_20] fold=2 model=SimCLR_base DONE ✅\n[80_20] fold=2 model=SimSiam_base -> extracting feats...\n[80_20] fold=2 model=SimSiam_base -> linear probe...\n[80_20] fold=2 model=SimSiam_base -> label efficiency...\n[80_20] fold=2 model=SimSiam_base DONE ✅\n[80_20] fold=2 model=Byol_base -> extracting feats...\n[80_20] fold=2 model=Byol_base -> linear probe...\n[80_20] fold=2 model=Byol_base -> label efficiency...\n[80_20] fold=2 model=Byol_base DONE ✅\n[80_20] fold=3 model=SimCLR_base -> extracting feats...\n[80_20] fold=3 model=SimCLR_base -> linear probe...\n[80_20] fold=3 model=SimCLR_base -> label efficiency...\n[80_20] fold=3 model=SimCLR_base DONE ✅\n[80_20] fold=3 model=SimSiam_base -> extracting feats...\n[80_20] fold=3 model=SimSiam_base -> linear probe...\n[80_20] fold=3 model=SimSiam_base -> label efficiency...\n[80_20] fold=3 model=SimSiam_base DONE ✅\n[80_20] fold=3 model=Byol_base -> extracting feats...\n[80_20] fold=3 model=Byol_base -> linear probe...\n[80_20] fold=3 model=Byol_base -> label efficiency...\n[80_20] fold=3 model=Byol_base DONE ✅\n[80_20] fold=4 model=SimCLR_base -> extracting feats...\n[80_20] fold=4 model=SimCLR_base -> linear probe...\n[80_20] fold=4 model=SimCLR_base -> label efficiency...\n[80_20] fold=4 model=SimCLR_base DONE ✅\n[80_20] fold=4 model=SimSiam_base -> extracting feats...\n[80_20] fold=4 model=SimSiam_base -> linear probe...\n[80_20] fold=4 model=SimSiam_base -> label efficiency...\n[80_20] fold=4 model=SimSiam_base DONE ✅\n[80_20] fold=4 model=Byol_base -> extracting feats...\n[80_20] fold=4 model=Byol_base -> linear probe...\n[80_20] fold=4 model=Byol_base -> label efficiency...\n[80_20] fold=4 model=Byol_base DONE ✅\n[70_30] fold=0 model=SimCLR_base -> extracting feats...\n[70_30] fold=0 model=SimCLR_base -> linear probe...\n[70_30] fold=0 model=SimCLR_base -> label efficiency...\n[70_30] fold=0 model=SimCLR_base DONE ✅\n[70_30] fold=0 model=SimSiam_base -> extracting feats...\n[70_30] fold=0 model=SimSiam_base -> linear probe...\n[70_30] fold=0 model=SimSiam_base -> label efficiency...\n[70_30] fold=0 model=SimSiam_base DONE ✅\n[70_30] fold=0 model=Byol_base -> extracting feats...\n[70_30] fold=0 model=Byol_base -> linear probe...\n[70_30] fold=0 model=Byol_base -> label efficiency...\n[70_30] fold=0 model=Byol_base DONE ✅\n[70_30] fold=1 model=SimCLR_base -> extracting feats...\n[70_30] fold=1 model=SimCLR_base -> linear probe...\n[70_30] fold=1 model=SimCLR_base -> label efficiency...\n[70_30] fold=1 model=SimCLR_base DONE ✅\n[70_30] fold=1 model=SimSiam_base -> extracting feats...\n[70_30] fold=1 model=SimSiam_base -> linear probe...\n[70_30] fold=1 model=SimSiam_base -> label efficiency...\n[70_30] fold=1 model=SimSiam_base DONE ✅\n[70_30] fold=1 model=Byol_base -> extracting feats...\n[70_30] fold=1 model=Byol_base -> linear probe...\n[70_30] fold=1 model=Byol_base -> label efficiency...\n[70_30] fold=1 model=Byol_base DONE ✅\n[70_30] fold=2 model=SimCLR_base -> extracting feats...\n[70_30] fold=2 model=SimCLR_base -> linear probe...\n[70_30] fold=2 model=SimCLR_base -> label efficiency...\n[70_30] fold=2 model=SimCLR_base DONE ✅\n[70_30] fold=2 model=SimSiam_base -> extracting feats...\n[70_30] fold=2 model=SimSiam_base -> linear probe...\n[70_30] fold=2 model=SimSiam_base -> label efficiency...\n[70_30] fold=2 model=SimSiam_base DONE ✅\n[70_30] fold=2 model=Byol_base -> extracting feats...\n[70_30] fold=2 model=Byol_base -> linear probe...\n[70_30] fold=2 model=Byol_base -> label efficiency...\n[70_30] fold=2 model=Byol_base DONE ✅\n[70_30] fold=3 model=SimCLR_base -> extracting feats...\n[70_30] fold=3 model=SimCLR_base -> linear probe...\n[70_30] fold=3 model=SimCLR_base -> label efficiency...\n[70_30] fold=3 model=SimCLR_base DONE ✅\n[70_30] fold=3 model=SimSiam_base -> extracting feats...\n[70_30] fold=3 model=SimSiam_base -> linear probe...\n[70_30] fold=3 model=SimSiam_base -> label efficiency...\n[70_30] fold=3 model=SimSiam_base DONE ✅\n[70_30] fold=3 model=Byol_base -> extracting feats...\n[70_30] fold=3 model=Byol_base -> linear probe...\n[70_30] fold=3 model=Byol_base -> label efficiency...\n[70_30] fold=3 model=Byol_base DONE ✅\n[70_30] fold=4 model=SimCLR_base -> extracting feats...\n[70_30] fold=4 model=SimCLR_base -> linear probe...\n[70_30] fold=4 model=SimCLR_base -> label efficiency...\n[70_30] fold=4 model=SimCLR_base DONE ✅\n[70_30] fold=4 model=SimSiam_base -> extracting feats...\n[70_30] fold=4 model=SimSiam_base -> linear probe...\n[70_30] fold=4 model=SimSiam_base -> label efficiency...\n[70_30] fold=4 model=SimSiam_base DONE ✅\n[70_30] fold=4 model=Byol_base -> extracting feats...\n[70_30] fold=4 model=Byol_base -> linear probe...\n[70_30] fold=4 model=Byol_base -> label efficiency...\n[70_30] fold=4 model=Byol_base DONE ✅\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    grid  test_size  fold         model       acc  f1_macro\n0  90_10        0.1     0   SimCLR_base  0.833333  0.817103\n1  90_10        0.1     0  SimSiam_base  0.466667  0.328483\n2  90_10        0.1     0     Byol_base  0.822222  0.757541\n3  90_10        0.1     1   SimCLR_base  0.811111  0.790707\n4  90_10        0.1     1  SimSiam_base  0.377778  0.182796","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>test_size</th>\n      <th>fold</th>\n      <th>model</th>\n      <th>acc</th>\n      <th>f1_macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimCLR_base</td>\n      <td>0.833333</td>\n      <td>0.817103</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimSiam_base</td>\n      <td>0.466667</td>\n      <td>0.328483</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>Byol_base</td>\n      <td>0.822222</td>\n      <td>0.757541</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>SimCLR_base</td>\n      <td>0.811111</td>\n      <td>0.790707</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>SimSiam_base</td>\n      <td>0.377778</td>\n      <td>0.182796</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    grid  test_size  fold         model       acc  f1_macro  frac\n0  90_10        0.1     0   SimCLR_base  0.755556  0.686417   0.1\n1  90_10        0.1     0   SimCLR_base  0.722222  0.613734   0.5\n2  90_10        0.1     0  SimSiam_base  0.377778  0.182796   0.1\n3  90_10        0.1     0  SimSiam_base  0.377778  0.182796   0.5\n4  90_10        0.1     0     Byol_base  0.688889  0.521981   0.1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>test_size</th>\n      <th>fold</th>\n      <th>model</th>\n      <th>acc</th>\n      <th>f1_macro</th>\n      <th>frac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimCLR_base</td>\n      <td>0.755556</td>\n      <td>0.686417</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimCLR_base</td>\n      <td>0.722222</td>\n      <td>0.613734</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimSiam_base</td>\n      <td>0.377778</td>\n      <td>0.182796</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>SimSiam_base</td>\n      <td>0.377778</td>\n      <td>0.182796</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>Byol_base</td>\n      <td>0.688889</td>\n      <td>0.521981</td>\n      <td>0.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"LP rows: 45 LE rows: 90\n","output_type":"stream"}],"execution_count":80},{"id":"de7c3aa5","cell_type":"markdown","source":"## 8) Ablation summary tables (mean ± std)\n\nThese tables are what you’ll paste into your report.\n","metadata":{}},{"id":"dd2e43fd","cell_type":"code","source":"def mean_std(df, group_cols, metric):\n    agg = df.groupby(group_cols)[metric].agg([\"mean\",\"std\"]).reset_index()\n    agg[\"mean±std\"] = agg[\"mean\"].map(lambda x: f\"{x:.4f}\") + \" ± \" + agg[\"std\"].map(lambda x: f\"{x:.4f}\")\n    return agg\n\nlp_table = mean_std(results_lp, [\"grid\",\"model\"], \"acc\").merge(\n    mean_std(results_lp, [\"grid\",\"model\"], \"f1_macro\")[[\"grid\",\"model\",\"mean±std\"]],\n    on=[\"grid\",\"model\"], suffixes=(\"_acc\",\"_f1\")\n).rename(columns={\"mean±std_acc\":\"acc_mean±std\",\"mean±std_f1\":\"f1_macro_mean±std\"})\n\nle_table = mean_std(results_le, [\"grid\",\"frac\",\"model\"], \"acc\").rename(columns={\"mean±std\":\"acc_mean±std\"})\nle_table_f1 = mean_std(results_le, [\"grid\",\"frac\",\"model\"], \"f1_macro\").rename(columns={\"mean±std\":\"f1_macro_mean±std\"})\nle_table = le_table.merge(le_table_f1[[\"grid\",\"frac\",\"model\",\"f1_macro_mean±std\"]], on=[\"grid\",\"frac\",\"model\"])\n\ndisplay(lp_table)\ndisplay(le_table.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:35:31.503336Z","iopub.execute_input":"2025-12-16T10:35:31.504024Z","iopub.status.idle":"2025-12-16T10:35:31.563550Z","shell.execute_reply.started":"2025-12-16T10:35:31.503994Z","shell.execute_reply":"2025-12-16T10:35:31.562969Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"    grid         model      mean       std     acc_mean±std f1_macro_mean±std\n0  70_30     Byol_base  0.753731  0.024610  0.7537 ± 0.0246   0.6070 ± 0.0590\n1  70_30   SimCLR_base  0.800746  0.010750  0.8007 ± 0.0107   0.7588 ± 0.0158\n2  70_30  SimSiam_base  0.381343  0.006675  0.3813 ± 0.0067   0.2090 ± 0.0412\n3  80_20     Byol_base  0.776536  0.023370  0.7765 ± 0.0234   0.6523 ± 0.0680\n4  80_20   SimCLR_base  0.824581  0.006370  0.8246 ± 0.0064   0.7891 ± 0.0112\n5  80_20  SimSiam_base  0.378771  0.009180  0.3788 ± 0.0092   0.2088 ± 0.0387\n6  90_10     Byol_base  0.788889  0.036851  0.7889 ± 0.0369   0.6766 ± 0.0787\n7  90_10   SimCLR_base  0.822222  0.007857  0.8222 ± 0.0079   0.8049 ± 0.0094\n8  90_10  SimSiam_base  0.428889  0.048177  0.4289 ± 0.0482   0.2712 ± 0.0808","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>model</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>acc_mean±std</th>\n      <th>f1_macro_mean±std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70_30</td>\n      <td>Byol_base</td>\n      <td>0.753731</td>\n      <td>0.024610</td>\n      <td>0.7537 ± 0.0246</td>\n      <td>0.6070 ± 0.0590</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>70_30</td>\n      <td>SimCLR_base</td>\n      <td>0.800746</td>\n      <td>0.010750</td>\n      <td>0.8007 ± 0.0107</td>\n      <td>0.7588 ± 0.0158</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70_30</td>\n      <td>SimSiam_base</td>\n      <td>0.381343</td>\n      <td>0.006675</td>\n      <td>0.3813 ± 0.0067</td>\n      <td>0.2090 ± 0.0412</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80_20</td>\n      <td>Byol_base</td>\n      <td>0.776536</td>\n      <td>0.023370</td>\n      <td>0.7765 ± 0.0234</td>\n      <td>0.6523 ± 0.0680</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>80_20</td>\n      <td>SimCLR_base</td>\n      <td>0.824581</td>\n      <td>0.006370</td>\n      <td>0.8246 ± 0.0064</td>\n      <td>0.7891 ± 0.0112</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>80_20</td>\n      <td>SimSiam_base</td>\n      <td>0.378771</td>\n      <td>0.009180</td>\n      <td>0.3788 ± 0.0092</td>\n      <td>0.2088 ± 0.0387</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>90_10</td>\n      <td>Byol_base</td>\n      <td>0.788889</td>\n      <td>0.036851</td>\n      <td>0.7889 ± 0.0369</td>\n      <td>0.6766 ± 0.0787</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>90_10</td>\n      <td>SimCLR_base</td>\n      <td>0.822222</td>\n      <td>0.007857</td>\n      <td>0.8222 ± 0.0079</td>\n      <td>0.8049 ± 0.0094</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>90_10</td>\n      <td>SimSiam_base</td>\n      <td>0.428889</td>\n      <td>0.048177</td>\n      <td>0.4289 ± 0.0482</td>\n      <td>0.2712 ± 0.0808</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     grid  frac         model      mean       std     acc_mean±std  \\\n0   70_30   0.1     Byol_base  0.719403  0.046932  0.7194 ± 0.0469   \n1   70_30   0.1   SimCLR_base  0.665672  0.059736  0.6657 ± 0.0597   \n2   70_30   0.1  SimSiam_base  0.379104  0.003337  0.3791 ± 0.0033   \n3   70_30   0.5     Byol_base  0.694030  0.090288  0.6940 ± 0.0903   \n4   70_30   0.5   SimCLR_base  0.748507  0.038471  0.7485 ± 0.0385   \n5   70_30   0.5  SimSiam_base  0.403731  0.055995  0.4037 ± 0.0560   \n6   80_20   0.1     Byol_base  0.716201  0.039186  0.7162 ± 0.0392   \n7   80_20   0.1   SimCLR_base  0.712849  0.075429  0.7128 ± 0.0754   \n8   80_20   0.1  SimSiam_base  0.379888  0.000000  0.3799 ± 0.0000   \n9   80_20   0.5     Byol_base  0.719553  0.051263  0.7196 ± 0.0513   \n10  80_20   0.5   SimCLR_base  0.746369  0.039622  0.7464 ± 0.0396   \n11  80_20   0.5  SimSiam_base  0.379888  0.000000  0.3799 ± 0.0000   \n12  90_10   0.1     Byol_base  0.733333  0.026058  0.7333 ± 0.0261   \n13  90_10   0.1   SimCLR_base  0.720000  0.058479  0.7200 ± 0.0585   \n14  90_10   0.1  SimSiam_base  0.377778  0.000000  0.3778 ± 0.0000   \n15  90_10   0.5     Byol_base  0.588889  0.158504  0.5889 ± 0.1585   \n16  90_10   0.5   SimCLR_base  0.702222  0.019876  0.7022 ± 0.0199   \n17  90_10   0.5  SimSiam_base  0.377778  0.000000  0.3778 ± 0.0000   \n\n   f1_macro_mean±std  \n0    0.5717 ± 0.0715  \n1    0.5556 ± 0.0955  \n2    0.1833 ± 0.0012  \n3    0.5317 ± 0.0748  \n4    0.6572 ± 0.0758  \n5    0.2210 ± 0.0846  \n6    0.5472 ± 0.0325  \n7    0.5972 ± 0.0988  \n8    0.1835 ± 0.0000  \n9    0.5563 ± 0.0469  \n10   0.6610 ± 0.0736  \n11   0.1835 ± 0.0000  \n12   0.5647 ± 0.0245  \n13   0.6366 ± 0.0950  \n14   0.1828 ± 0.0000  \n15   0.4280 ± 0.1620  \n16   0.5999 ± 0.0753  \n17   0.1828 ± 0.0000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>frac</th>\n      <th>model</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>acc_mean±std</th>\n      <th>f1_macro_mean±std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70_30</td>\n      <td>0.1</td>\n      <td>Byol_base</td>\n      <td>0.719403</td>\n      <td>0.046932</td>\n      <td>0.7194 ± 0.0469</td>\n      <td>0.5717 ± 0.0715</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>70_30</td>\n      <td>0.1</td>\n      <td>SimCLR_base</td>\n      <td>0.665672</td>\n      <td>0.059736</td>\n      <td>0.6657 ± 0.0597</td>\n      <td>0.5556 ± 0.0955</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70_30</td>\n      <td>0.1</td>\n      <td>SimSiam_base</td>\n      <td>0.379104</td>\n      <td>0.003337</td>\n      <td>0.3791 ± 0.0033</td>\n      <td>0.1833 ± 0.0012</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70_30</td>\n      <td>0.5</td>\n      <td>Byol_base</td>\n      <td>0.694030</td>\n      <td>0.090288</td>\n      <td>0.6940 ± 0.0903</td>\n      <td>0.5317 ± 0.0748</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>70_30</td>\n      <td>0.5</td>\n      <td>SimCLR_base</td>\n      <td>0.748507</td>\n      <td>0.038471</td>\n      <td>0.7485 ± 0.0385</td>\n      <td>0.6572 ± 0.0758</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>70_30</td>\n      <td>0.5</td>\n      <td>SimSiam_base</td>\n      <td>0.403731</td>\n      <td>0.055995</td>\n      <td>0.4037 ± 0.0560</td>\n      <td>0.2210 ± 0.0846</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>80_20</td>\n      <td>0.1</td>\n      <td>Byol_base</td>\n      <td>0.716201</td>\n      <td>0.039186</td>\n      <td>0.7162 ± 0.0392</td>\n      <td>0.5472 ± 0.0325</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>80_20</td>\n      <td>0.1</td>\n      <td>SimCLR_base</td>\n      <td>0.712849</td>\n      <td>0.075429</td>\n      <td>0.7128 ± 0.0754</td>\n      <td>0.5972 ± 0.0988</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>80_20</td>\n      <td>0.1</td>\n      <td>SimSiam_base</td>\n      <td>0.379888</td>\n      <td>0.000000</td>\n      <td>0.3799 ± 0.0000</td>\n      <td>0.1835 ± 0.0000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>80_20</td>\n      <td>0.5</td>\n      <td>Byol_base</td>\n      <td>0.719553</td>\n      <td>0.051263</td>\n      <td>0.7196 ± 0.0513</td>\n      <td>0.5563 ± 0.0469</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>80_20</td>\n      <td>0.5</td>\n      <td>SimCLR_base</td>\n      <td>0.746369</td>\n      <td>0.039622</td>\n      <td>0.7464 ± 0.0396</td>\n      <td>0.6610 ± 0.0736</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>80_20</td>\n      <td>0.5</td>\n      <td>SimSiam_base</td>\n      <td>0.379888</td>\n      <td>0.000000</td>\n      <td>0.3799 ± 0.0000</td>\n      <td>0.1835 ± 0.0000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>Byol_base</td>\n      <td>0.733333</td>\n      <td>0.026058</td>\n      <td>0.7333 ± 0.0261</td>\n      <td>0.5647 ± 0.0245</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>SimCLR_base</td>\n      <td>0.720000</td>\n      <td>0.058479</td>\n      <td>0.7200 ± 0.0585</td>\n      <td>0.6366 ± 0.0950</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>90_10</td>\n      <td>0.1</td>\n      <td>SimSiam_base</td>\n      <td>0.377778</td>\n      <td>0.000000</td>\n      <td>0.3778 ± 0.0000</td>\n      <td>0.1828 ± 0.0000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>90_10</td>\n      <td>0.5</td>\n      <td>Byol_base</td>\n      <td>0.588889</td>\n      <td>0.158504</td>\n      <td>0.5889 ± 0.1585</td>\n      <td>0.4280 ± 0.1620</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>90_10</td>\n      <td>0.5</td>\n      <td>SimCLR_base</td>\n      <td>0.702222</td>\n      <td>0.019876</td>\n      <td>0.7022 ± 0.0199</td>\n      <td>0.5999 ± 0.0753</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>90_10</td>\n      <td>0.5</td>\n      <td>SimSiam_base</td>\n      <td>0.377778</td>\n      <td>0.000000</td>\n      <td>0.3778 ± 0.0000</td>\n      <td>0.1828 ± 0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":81},{"id":"948df6f4","cell_type":"markdown","source":"## 9) Statistics\n\nRules (as in prompt):\n- **Paired t-test** for 2-model comparisons on identical folds\n- **Friedman** for ≥3 models, then **Nemenyi post-hoc** (pairwise)\n\nWe compute stats separately for each grid setting.\n","metadata":{}},{"id":"c82fbc90","cell_type":"code","source":"def paired_ttest(a, b):\n    # a,b are same-length arrays on identical folds\n    t, p = stats.ttest_rel(a, b)\n    return float(t), float(p)\n\ndef friedman_test(matrix):\n    # matrix: shape (n_blocks, k_models)\n    # scipy wants k separate args\n    args = [matrix[:,j] for j in range(matrix.shape[1])]\n    stat, p = stats.friedmanchisquare(*args)\n    return float(stat), float(p)\n\ndef nemenyi_posthoc(matrix, model_names):\n    # Based on average ranks; p-values via studentized range distribution (statsmodels psturng)\n    n, k = matrix.shape\n    # ranks per row (higher metric => better rank 1)\n    ranks = np.array([stats.rankdata(-row, method=\"average\") for row in matrix])\n    avg_ranks = ranks.mean(axis=0)\n    denom = math.sqrt(k*(k+1)/(6*n))\n    pvals = pd.DataFrame(np.ones((k,k)), index=model_names, columns=model_names)\n    for i in range(k):\n        for j in range(i+1,k):\n            q = abs(avg_ranks[i]-avg_ranks[j]) / denom\n            p = float(1 - psturng(q, k, np.inf))  # asymptotic\n            pvals.iloc[i,j] = pvals.iloc[j,i] = p\n    return avg_ranks, pvals\n\ndef cohen_d_paired(a,b):\n    d = (np.mean(a-b)) / (np.std(a-b, ddof=1) + 1e-12)\n    return float(d)\n\n# --- Example: compare BASE vs ABL for SimCLR if you provide both ---\n# Adjust these names to match encoders keys you actually have.\nCOMPARES = [\n    (\"SimCLR_base\", \"SimCLR_abl\"),\n    (\"SimSiam_base\", \"SimSiam_abl\"),\n    (\"Byol_base\", \"Byol_abl\"),\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:53:50.290604Z","iopub.execute_input":"2025-12-16T10:53:50.290911Z","iopub.status.idle":"2025-12-16T10:53:50.298303Z","shell.execute_reply.started":"2025-12-16T10:53:50.290888Z","shell.execute_reply":"2025-12-16T10:53:50.297530Z"}},"outputs":[],"execution_count":168},{"id":"b5f65594","cell_type":"code","source":"stats_rows = []\n\nfor grid_name in results_lp[\"grid\"].unique():\n    df = results_lp[results_lp[\"grid\"]==grid_name]\n    # 1) paired t-tests for requested pairs\n    for a,b in COMPARES:\n        if a in df[\"model\"].unique() and b in df[\"model\"].unique():\n            A = df[df[\"model\"]==a].sort_values(\"fold\")[\"acc\"].to_numpy()\n            B = df[df[\"model\"]==b].sort_values(\"fold\")[\"acc\"].to_numpy()\n            t,p = paired_ttest(A,B)\n            d = cohen_d_paired(A,B)\n            stats_rows.append({\"grid\": grid_name, \"test\": \"paired_t\", \"metric\":\"acc\", \"A\":a, \"B\":b, \"t\":t, \"p\":p, \"cohen_d\":d})\n\n    # 2) Friedman + Nemenyi (all models present)\n    models = sorted(df[\"model\"].unique())\n    mat = []\n    for fold in sorted(df[\"fold\"].unique()):\n        row=[]\n        for m in models:\n            row.append(df[(df[\"fold\"]==fold) & (df[\"model\"]==m)][\"acc\"].iloc[0])\n        mat.append(row)\n    mat = np.array(mat)\n    if mat.shape[1] >= 3:\n        chi2,p = friedman_test(mat)\n        stats_rows.append({\"grid\": grid_name, \"test\":\"friedman\", \"metric\":\"acc\", \"chi2\":chi2, \"p\":p, \"models\":\", \".join(models)})\n        avg_ranks, pvals = nemenyi_posthoc(mat, models)\n        display(pd.DataFrame({\"model\":models, \"avg_rank\":avg_ranks}).sort_values(\"avg_rank\"))\n        display(pvals)\n\nstats_df = pd.DataFrame(stats_rows)\nstats_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:53:56.013335Z","iopub.execute_input":"2025-12-16T10:53:56.013591Z","iopub.status.idle":"2025-12-16T10:53:56.142402Z","shell.execute_reply.started":"2025-12-16T10:53:56.013573Z","shell.execute_reply":"2025-12-16T10:53:56.141827Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/1475224057.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p = float(1 - psturng(q, k, np.inf))  # asymptotic\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          model  avg_rank\n1   SimCLR_base       1.1\n0     Byol_base       1.9\n2  SimSiam_base       3.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>avg_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>SimCLR_base</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Byol_base</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SimSiam_base</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"              Byol_base  SimCLR_base  SimSiam_base\nByol_base      1.000000     0.368144      0.562593\nSimCLR_base    0.368144     1.000000      0.914951\nSimSiam_base   0.562593     0.914951      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Byol_base</th>\n      <th>SimCLR_base</th>\n      <th>SimSiam_base</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Byol_base</th>\n      <td>1.000000</td>\n      <td>0.368144</td>\n      <td>0.562593</td>\n    </tr>\n    <tr>\n      <th>SimCLR_base</th>\n      <td>0.368144</td>\n      <td>1.000000</td>\n      <td>0.914951</td>\n    </tr>\n    <tr>\n      <th>SimSiam_base</th>\n      <td>0.562593</td>\n      <td>0.914951</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/1475224057.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p = float(1 - psturng(q, k, np.inf))  # asymptotic\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          model  avg_rank\n1   SimCLR_base       1.0\n0     Byol_base       2.0\n2  SimSiam_base       3.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>avg_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>SimCLR_base</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Byol_base</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SimSiam_base</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"              Byol_base  SimCLR_base  SimSiam_base\nByol_base      1.000000     0.496058      0.496058\nSimCLR_base    0.496058     1.000000      0.934697\nSimSiam_base   0.496058     0.934697      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Byol_base</th>\n      <th>SimCLR_base</th>\n      <th>SimSiam_base</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Byol_base</th>\n      <td>1.000000</td>\n      <td>0.496058</td>\n      <td>0.496058</td>\n    </tr>\n    <tr>\n      <th>SimCLR_base</th>\n      <td>0.496058</td>\n      <td>1.000000</td>\n      <td>0.934697</td>\n    </tr>\n    <tr>\n      <th>SimSiam_base</th>\n      <td>0.496058</td>\n      <td>0.934697</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/1475224057.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p = float(1 - psturng(q, k, np.inf))  # asymptotic\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          model  avg_rank\n1   SimCLR_base       1.0\n0     Byol_base       2.0\n2  SimSiam_base       3.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>avg_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>SimCLR_base</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Byol_base</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SimSiam_base</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"              Byol_base  SimCLR_base  SimSiam_base\nByol_base      1.000000     0.496058      0.496058\nSimCLR_base    0.496058     1.000000      0.934697\nSimSiam_base   0.496058     0.934697      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Byol_base</th>\n      <th>SimCLR_base</th>\n      <th>SimSiam_base</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Byol_base</th>\n      <td>1.000000</td>\n      <td>0.496058</td>\n      <td>0.496058</td>\n    </tr>\n    <tr>\n      <th>SimCLR_base</th>\n      <td>0.496058</td>\n      <td>1.000000</td>\n      <td>0.934697</td>\n    </tr>\n    <tr>\n      <th>SimSiam_base</th>\n      <td>0.496058</td>\n      <td>0.934697</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"    grid      test metric       chi2         p  \\\n0  90_10  friedman    acc   9.578947  0.008317   \n1  80_20  friedman    acc  10.000000  0.006738   \n2  70_30  friedman    acc  10.000000  0.006738   \n\n                                 models  \n0  Byol_base, SimCLR_base, SimSiam_base  \n1  Byol_base, SimCLR_base, SimSiam_base  \n2  Byol_base, SimCLR_base, SimSiam_base  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>test</th>\n      <th>metric</th>\n      <th>chi2</th>\n      <th>p</th>\n      <th>models</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90_10</td>\n      <td>friedman</td>\n      <td>acc</td>\n      <td>9.578947</td>\n      <td>0.008317</td>\n      <td>Byol_base, SimCLR_base, SimSiam_base</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80_20</td>\n      <td>friedman</td>\n      <td>acc</td>\n      <td>10.000000</td>\n      <td>0.006738</td>\n      <td>Byol_base, SimCLR_base, SimSiam_base</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70_30</td>\n      <td>friedman</td>\n      <td>acc</td>\n      <td>10.000000</td>\n      <td>0.006738</td>\n      <td>Byol_base, SimCLR_base, SimSiam_base</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":169},{"id":"e2443298","cell_type":"markdown","source":"## 10) Practical vs statistical significance + effect size figures\n\nWe plot mean difference (Δ accuracy) with 95% CI for key comparisons (baseline vs ablation).\n","metadata":{}},{"id":"531ca85c-e28e-46fd-a95e-b003a5207584","cell_type":"code","source":"COMPARES = [\n    (\"SimCLR_base\", \"SimSiam_base\"),\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:56:17.237901Z","iopub.execute_input":"2025-12-16T10:56:17.238179Z","iopub.status.idle":"2025-12-16T10:56:17.241666Z","shell.execute_reply.started":"2025-12-16T10:56:17.238158Z","shell.execute_reply":"2025-12-16T10:56:17.241083Z"}},"outputs":[],"execution_count":185},{"id":"e7104263","cell_type":"code","source":"def mean_diff_ci(a,b,alpha=0.05):\n    diff = a-b\n    m = diff.mean()\n    se = diff.std(ddof=1)/math.sqrt(len(diff))\n    tcrit = stats.t.ppf(1-alpha/2, df=len(diff)-1)\n    return float(m), float(m - tcrit*se), float(m + tcrit*se)\n\nfig_rows=[]\nfor grid_name in results_lp[\"grid\"].unique():\n    df = results_lp[results_lp[\"grid\"]==grid_name]\n    for a,b in COMPARES:\n        if a in df[\"model\"].unique() and b in df[\"model\"].unique():\n            A = df[df[\"model\"]==a].sort_values(\"fold\")[\"acc\"].to_numpy()\n            B = df[df[\"model\"]==b].sort_values(\"fold\")[\"acc\"].to_numpy()\n            md, lo, hi = mean_diff_ci(A,B)\n            fig_rows.append({\"grid\":grid_name, \"pair\":f\"{a} - {b}\", \"mean_diff\":md, \"ci_lo\":lo, \"ci_hi\":hi})\n\nfig_df = pd.DataFrame(fig_rows)\nfig_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:56:21.526395Z","iopub.execute_input":"2025-12-16T10:56:21.527087Z","iopub.status.idle":"2025-12-16T10:56:21.548377Z","shell.execute_reply.started":"2025-12-16T10:56:21.527061Z","shell.execute_reply":"2025-12-16T10:56:21.547710Z"}},"outputs":[{"execution_count":186,"output_type":"execute_result","data":{"text/plain":"    grid                        pair  mean_diff     ci_lo     ci_hi\n0  90_10  SimCLR_base - SimSiam_base   0.393333  0.339369  0.447298\n1  80_20  SimCLR_base - SimSiam_base   0.445810  0.429992  0.461628\n2  70_30  SimCLR_base - SimSiam_base   0.419403  0.402192  0.436614","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>pair</th>\n      <th>mean_diff</th>\n      <th>ci_lo</th>\n      <th>ci_hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90_10</td>\n      <td>SimCLR_base - SimSiam_base</td>\n      <td>0.393333</td>\n      <td>0.339369</td>\n      <td>0.447298</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80_20</td>\n      <td>SimCLR_base - SimSiam_base</td>\n      <td>0.445810</td>\n      <td>0.429992</td>\n      <td>0.461628</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70_30</td>\n      <td>SimCLR_base - SimSiam_base</td>\n      <td>0.419403</td>\n      <td>0.402192</td>\n      <td>0.436614</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":186},{"id":"9f835be7","cell_type":"code","source":"# Figure 1: Mean Δacc with 95% CI (baseline - ablation)\nif len(fig_df):\n    plt.figure(figsize=(8,4))\n    x = np.arange(len(fig_df))\n    y = fig_df[\"mean_diff\"].to_numpy()\n    yerr = np.vstack([y - fig_df[\"ci_lo\"].to_numpy(), fig_df[\"ci_hi\"].to_numpy() - y])\n    plt.errorbar(x, y, yerr=yerr, fmt=\"o\", capsize=4)\n    plt.axhline(0, linestyle=\"--\")\n    plt.xticks(x, fig_df[\"grid\"] + \"\\n\" + fig_df[\"pair\"], rotation=20, ha=\"right\")\n    plt.ylabel(\"Δ accuracy (paired folds)\")\n    plt.title(\"Effect size (mean difference) with 95% CI\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No ablation pairs found in COMPARES yet.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:56:32.946924Z","iopub.execute_input":"2025-12-16T10:56:32.947203Z","iopub.status.idle":"2025-12-16T10:56:33.098977Z","shell.execute_reply.started":"2025-12-16T10:56:32.947180Z","shell.execute_reply":"2025-12-16T10:56:33.098041Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":187},{"id":"4e2264d9","cell_type":"markdown","source":"## 11) Final notes for your write-up\n\nIn your report section:\n- Mention **which grid** you used (same as Task 2) and **same folds** for paired comparisons.\n- For each SSL, describe the **single ablation** clearly (what changed and why it matters).\n- Discuss:\n  - If p-value is small but Δacc is tiny → statistically significant but not practically meaningful.\n  - If Δacc is big but p-value is marginal → likely underpowered; increase folds or repeats.\n\n---\n\n### What you still need to do (minimum)\n1) Add at least one encoder checkpoint per ablation:\n- `SimCLR_abl`\n- `SimSiam_abl`\n\n2) Update `COMPARES` list accordingly.\n\nThen re-run Task 7 and paste:\n- `lp_table`, `le_table`\n- `stats_df` + Nemenyi tables\n- Figure(s)\n","metadata":{}}]}