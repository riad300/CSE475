{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13416578,"sourceType":"datasetVersion","datasetId":8515242}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# Task 7 - Simsiam (2nd SSL)\n## Shourav Deb [2021-3-60-274]\n---","metadata":{}},{"cell_type":"markdown","source":"## CELL 1: Ablation - varying pretraining epochs (100, 200, 400)","metadata":{}},{"cell_type":"code","source":"import os, time, json, shutil\nimport torch\nfrom torch import optim\nfrom tqdm import tqdm\n\n\nablation_epochs = [100, 200, 400] \nsave_interval = 20              \nzip_every_k_backups = 1        \nzip_output_name_template = \"simsiam_ablation_epochs_{E}.zip\" \n\n\n# hyperparams\nlearning_rate = globals().get(\"learning_rate\", 0.05)\nmomentum = globals().get(\"momentum\", 0.9)\nweight_decay = globals().get(\"weight_decay\", 1e-4)\n\n# sanity checks\nif \"train_loader\" not in globals():\n    raise RuntimeError(\"train_loader not found\")\nif \"SimSiam\" not in globals():\n    raise RuntimeError(\"SimSiam class not defined\")\n\nOUT_DIR = globals().get(\"OUT_DIR\", \"/kaggle/working/simsiam_task4\")\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndevice = globals().get(\"DEVICE\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\nBACKBONE = globals().get(\"BACKBONE\", \"resnet18\")\n\n# negative cosine similarity helper\ndef negative_cosine_similarity(p, z):\n    z = z.detach()\n    p = torch.nn.functional.normalize(p, dim=1)\n    z = torch.nn.functional.normalize(z, dim=1)\n    return - (p * z).sum(dim=1).mean()\n\n# save checkpoint\ndef save_full_checkpoint(model, optimizer, scheduler, run_dir, epoch_num, avg_loss):\n    epoch_dir = os.path.join(run_dir, f\"epoch_{epoch_num:03d}\")\n    os.makedirs(epoch_dir, exist_ok=True)\n    ck = {\n        \"epoch\": int(epoch_num),\n        \"timestamp\": time.time(),\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict() if optimizer is not None else None,\n        \"scheduler_state\": scheduler.state_dict() if scheduler is not None else None,\n        \"avg_loss\": float(avg_loss),\n        \"manifest\": globals().get(\"split_manifest\", None)\n    }\n    ck_path = os.path.join(epoch_dir, \"checkpoint.pth\")\n    torch.save(ck, ck_path)\n\n    try:\n        enc_path = os.path.join(epoch_dir, \"encoder.pth\")\n        torch.save({\"encoder_state_dict\": model.encoder.state_dict(), \"feat_dim\": model.feat_dim}, enc_path)\n    except Exception:\n        pass\n\n    meta = {\"epoch\": int(epoch_num), \"avg_loss\": float(avg_loss), \"saved_at\": time.time()}\n    with open(os.path.join(epoch_dir, \"metadata.json\"), \"w\") as f:\n        json.dump(meta, f)\n    return ck_path\n\n# Main ablation loop\nfor E in ablation_epochs:\n    run_dir = os.path.join(OUT_DIR, f\"ablation_epochs_{E}\")\n    os.makedirs(run_dir, exist_ok=True)\n    zip_out_path = os.path.join(\"/kaggle/working\", zip_output_name_template.format(E=E))\n\n    print(f\"\\n=== Ablation config: {E} epochs | saving every {save_interval} epochs | run_dir: {run_dir} ===\")\n\n\n    model_ab = SimSiam(backbone=BACKBONE).to(device)\n    opt_ab = optim.SGD(model_ab.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n    sched_ab = torch.optim.lr_scheduler.CosineAnnealingLR(opt_ab, T_max=E)\n\n\n    existing_epochs = []\n    if os.path.exists(run_dir):\n        for name in os.listdir(run_dir):\n            if name.startswith(\"epoch_\"):\n                try:\n                    existing_epochs.append(int(name.split(\"_\")[1]))\n                except Exception:\n                    pass\n    last_saved = max(existing_epochs) if existing_epochs else 0\n\n    start_epoch = last_saved + 1\n\n\n    if last_saved > 0:\n        ckpt_path = os.path.join(run_dir, f\"epoch_{last_saved:03d}\", \"checkpoint.pth\")\n        if os.path.exists(ckpt_path):\n            print(f\"Resuming from saved checkpoint at epoch {last_saved}: {ckpt_path}\")\n            ck = torch.load(ckpt_path, map_location=device, weights_only=False)\n            try:\n                model_ab.load_state_dict(ck[\"model_state\"])\n            except Exception as e:\n                print(\"Warning: model_state load partial/failed ->\", e)\n            try:\n                if ck.get(\"optimizer_state\") is not None:\n                    opt_ab.load_state_dict(ck[\"optimizer_state\"])\n            except Exception as e:\n                print(\"Warning: optimizer_state load failed ->\", e)\n            try:\n                if ck.get(\"scheduler_state\") is not None:\n                    sched_ab.load_state_dict(ck[\"scheduler_state\"])\n            except Exception:\n                pass\n            print(f\"Resumed. Next epoch will be {start_epoch} (1-based) out of {E}.\")\n        else:\n            print(\"Found epoch folders but checkpoint.pth missing.\")\n            start_epoch = 1\n    else:\n        start_epoch = 1\n\n    if last_saved > 0:\n        print(f\"Note: last saved epoch = {last_saved}. Progress after that may be lost if run stopped earlier than next save point.\")\n\n    \n    try:\n        for epoch in range(start_epoch, E + 1):\n            model_ab.train()\n            running_loss = 0.0\n            n_steps = 0\n\n            loop = tqdm(train_loader, desc=f\"Ablation E={E} Epoch {epoch}/{E}\", leave=False)\n            for x1, x2, _, _ in loop:\n                x1 = x1.to(device); x2 = x2.to(device)\n\n                p1, p2, z1, z2 = model_ab(x1, x2)\n                loss = 0.5 * negative_cosine_similarity(p1, z2) + 0.5 * negative_cosine_similarity(p2, z1)\n\n                opt_ab.zero_grad()\n                loss.backward()\n                opt_ab.step()\n\n                running_loss += loss.item()\n                n_steps += 1\n                loop.set_postfix(loss=f\"{loss.item():.4f}\")\n\n            avg_loss = (running_loss / n_steps) if n_steps > 0 else 0.0\n            sched_ab.step()\n            print(f\"Epoch {epoch}/{E} completed - Avg Loss: {avg_loss:.4f}\")\n\n            \n            to_save = (epoch % save_interval == 0) or (epoch == E)\n            if to_save:\n                try:\n                    ck_path = save_full_checkpoint(model_ab, opt_ab, sched_ab, run_dir, epoch, avg_loss)\n                    print(\"Saved full checkpoint for epoch:\", epoch, \"->\", ck_path)\n                except Exception as e:\n                    print(\"Warning: could not save checkpoint:\", e)\n\n                \n                if zip_every_k_backups and zip_every_k_backups > 0:\n                    try:\n                        \n                        if os.path.exists(zip_out_path):\n                            os.remove(zip_out_path)\n                        shutil.make_archive(base_name=zip_out_path.replace(\".zip\",\"\"), format=\"zip\", root_dir=run_dir)\n                        print(\"Saved ZIP backup to:\", zip_out_path)\n                    except Exception as e:\n                        print(\"Warning: ZIP backup failed ->\", e)\n\n    except KeyboardInterrupt:\n        \n        print(\"KeyboardInterrupt caught - attempting to save resume checkpoint.\")\n        try:\n            \n            cur_epoch = epoch\n            if (cur_epoch % save_interval == 0) or (cur_epoch == E):\n                save_full_checkpoint(model_ab, opt_ab, sched_ab, run_dir, cur_epoch, avg_loss if 'avg_loss' in locals() else 0.0)\n                print(\"Saved checkpoint for interrupted epoch:\", cur_epoch)\n            else:\n                print(f\"Current epoch {cur_epoch} not a save point. Last permanent save remains epoch {last_saved}.\")\n        except Exception as e:\n            print(\"Could not save on interrupt:\", e)\n        raise\n\n    \n    try:\n        if zip_every_k_backups and zip_every_k_backups > 0:\n            if os.path.exists(zip_out_path):\n                os.remove(zip_out_path)\n            shutil.make_archive(base_name=zip_out_path.replace(\".zip\",\"\"), format=\"zip\", root_dir=run_dir)\n            print(\"Completed ablation run. Final ZIP:\", zip_out_path)\n    except Exception as e:\n        print(\"Could not create final ZIP:\", e)\n\nprint(\"\\nAll ablation configs processed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T06:35:31.235997Z","iopub.execute_input":"2025-12-08T06:35:31.236595Z","iopub.status.idle":"2025-12-08T06:35:31.256258Z","shell.execute_reply.started":"2025-12-08T06:35:31.236565Z","shell.execute_reply":"2025-12-08T06:35:31.255557Z"}},"outputs":[{"name":"stdout","text":"\n=== Ablation config: 100 epochs | saving every 20 epochs | run_dir: /kaggle/working/simsiam_task4/ablation_epochs_100 ===\n\nEpoch 1/100 completed - Avg Loss: -0.3544\n                                                                                         \nEpoch 2/100 completed - Avg Loss: -0.6602\n                                                                                         \nEpoch 3/100 completed - Avg Loss: -0.8142\n                                                                                         \nEpoch 4/100 completed - Avg Loss: -0.8335\n                                                                                         \nEpoch 5/100 completed - Avg Loss: -0.8616\n                                                                                         \nEpoch 6/100 completed - Avg Loss: -0.8948\n                                                                                         \nEpoch 7/100 completed - Avg Loss: -0.8459\n                                                                                         \nEpoch 8/100 completed - Avg Loss: -0.8835\n                                                                                         \nEpoch 9/100 completed - Avg Loss: -0.9091\n                                                                                          \nEpoch 10/100 completed - Avg Loss: -0.9155\n                                                                                          \nEpoch 11/100 completed - Avg Loss: -0.9060\n                                                                                          \nEpoch 12/100 completed - Avg Loss: -0.9051\n                                                                                          \nEpoch 13/100 completed - Avg Loss: -0.9177\n                                                                                          \nEpoch 14/100 completed - Avg Loss: -0.9089\n                                                                                          \nEpoch 15/100 completed - Avg Loss: -0.9111\n                                                                                          \nEpoch 16/100 completed - Avg Loss: -0.9254\n                                                                                          \nEpoch 17/100 completed - Avg Loss: -0.9219\n                                                                                          \nEpoch 18/100 completed - Avg Loss: -0.9178\n                                                                                          \nEpoch 19/100 completed - Avg Loss: -0.8918\n                                                                                          \nEpoch 20/100 completed - Avg Loss: -0.9234\nSaved full checkpoint for epoch: 20 -> /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_020/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                                          \nEpoch 21/100 completed - Avg Loss: -0.9221\n                                                                                          \nEpoch 22/100 completed - Avg Loss: -0.9404\n                                                                                          \nEpoch 23/100 completed - Avg Loss: -0.9371\n                                                                                          \nEpoch 24/100 completed - Avg Loss: -0.9062\n                                                                                          \nEpoch 25/100 completed - Avg Loss: -0.8888\n                                                                                          \nEpoch 26/100 completed - Avg Loss: -0.9240\n                                                                                          \nEpoch 27/100 completed - Avg Loss: -0.9341\n                                                                                          \nEpoch 28/100 completed - Avg Loss: -0.9380\n                                                                                          \nEpoch 29/100 completed - Avg Loss: -0.9371\n                                                                                          \nEpoch 30/100 completed - Avg Loss: -0.9390\n                                                                                          \nEpoch 31/100 completed - Avg Loss: -0.9326\n                                                                                          \nEpoch 32/100 completed - Avg Loss: -0.9433\n                                                                                          \nEpoch 33/100 completed - Avg Loss: -0.9424\n                                                                                          \nEpoch 34/100 completed - Avg Loss: -0.9477\n                                                                                          \nEpoch 35/100 completed - Avg Loss: -0.9463\n                                                                                          \nEpoch 36/100 completed - Avg Loss: -0.9468\n                                                                                          \nEpoch 37/100 completed - Avg Loss: -0.9433\n                                                                                          \nEpoch 38/100 completed - Avg Loss: -0.9434\n                                                                                          \nEpoch 39/100 completed - Avg Loss: -0.9445\n                                                                                          \nEpoch 40/100 completed - Avg Loss: -0.9481\nSaved full checkpoint for epoch: 40 -> /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_040/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                                          \nEpoch 41/100 completed - Avg Loss: -0.9451\n                                                                                          \nEpoch 42/100 completed - Avg Loss: -0.9364\n                                                                                          \nEpoch 43/100 completed - Avg Loss: -0.9513\n                                                                                          \nEpoch 44/100 completed - Avg Loss: -0.9480\n                                                                                          \nEpoch 45/100 completed - Avg Loss: -0.9490\n                                                                                          \nEpoch 46/100 completed - Avg Loss: -0.9489\n                                                                                          \nEpoch 47/100 completed - Avg Loss: -0.9502\n                                                                                          \nEpoch 48/100 completed - Avg Loss: -0.9503\n                                                                                          \nEpoch 49/100 completed - Avg Loss: -0.9509\n                                                                                          \nEpoch 50/100 completed - Avg Loss: -0.9496\n                                                                                          \nEpoch 51/100 completed - Avg Loss: -0.9520\n                                                                                          \nEpoch 52/100 completed - Avg Loss: -0.9510\n                                                                                          \nEpoch 53/100 completed - Avg Loss: -0.9535\n                                                                                          \nEpoch 54/100 completed - Avg Loss: -0.9472\n                                                                                          \nEpoch 55/100 completed - Avg Loss: -0.9480\n                                                                                          \nEpoch 56/100 completed - Avg Loss: -0.9471\n                                                                                          \nEpoch 57/100 completed - Avg Loss: -0.9479\n                                                                                          \nEpoch 58/100 completed - Avg Loss: -0.9500\n                                                                                          \nEpoch 59/100 completed - Avg Loss: -0.9455\n                                                                                          \nEpoch 60/100 completed - Avg Loss: -0.9424\nSaved full checkpoint for epoch: 60 -> /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_060/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                                          \nEpoch 61/100 completed - Avg Loss: -0.9420\n                                                                                          \nEpoch 62/100 completed - Avg Loss: -0.9426\n                                                                                          \nEpoch 63/100 completed - Avg Loss: -0.9363\n                                                                                          \nEpoch 64/100 completed - Avg Loss: -0.9406\n                                                                                          \nEpoch 65/100 completed - Avg Loss: -0.9366\n                                                                                          \nEpoch 66/100 completed - Avg Loss: -0.9419\n                                                                                          \nEpoch 67/100 completed - Avg Loss: -0.9453\n                                                                                          \nEpoch 68/100 completed - Avg Loss: -0.9445\n                                                                                          \nEpoch 69/100 completed - Avg Loss: -0.9445\n                                                                                          \nEpoch 71/100 completed - Avg Loss: -0.9411\n                                                                                          \nEpoch 72/100 completed - Avg Loss: -0.9436\n                                                                                          \nEpoch 73/100 completed - Avg Loss: -0.9392\n                                                                                          \nEpoch 74/100 completed - Avg Loss: -0.9386\n                                                                                          \nEpoch 75/100 completed - Avg Loss: -0.9390\n                                                                                          \nEpoch 76/100 completed - Avg Loss: -0.9381\n                                                                                          \nEpoch 77/100 completed - Avg Loss: -0.9356\n                                                                                          \nEpoch 78/100 completed - Avg Loss: -0.9323\n                                                                                          \nEpoch 79/100 completed - Avg Loss: -0.9391       \n                                                                                          \nEpoch 80/100 completed - Avg Loss: -0.9388\n\nEpoch 81/100 completed - Avg Loss: -0.9401\n                                                                           \nEpoch 82/100 completed - Avg Loss: -0.9332\n                                                                           \nEpoch 83/100 completed - Avg Loss: -0.9370\n                                                                           \nEpoch 84/100 completed - Avg Loss: -0.9354\n                                                                           \nEpoch 85/100 completed - Avg Loss: -0.9359\nSaved full checkpoint for epoch: 85 ->  /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_085/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                           \nEpoch 86/100 completed - Avg Loss: -0.9361\n                                                                           \nEpoch 87/100 completed - Avg Loss: -0.9341\n                                                                           \nEpoch 88/100 completed - Avg Loss: -0.9346\n                                                                           \nEpoch 89/100 completed - Avg Loss: -0.9357\n                                                                           \nEpoch 90/100 completed - Avg Loss: -0.9347\nSaved full checkpoint for epoch: 90 ->  /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_090/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                           \nEpoch 91/100 completed - Avg Loss: -0.9339\n                                                                           \nEpoch 92/100 completed - Avg Loss: -0.9330\n                                                                           \nEpoch 93/100 completed - Avg Loss: -0.9331\n                                                                           \nEpoch 94/100 completed - Avg Loss: -0.9349\n                                                                           \nEpoch 95/100 completed - Avg Loss: -0.9344\nSaved full checkpoint for epoch: 95 ->  /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_095/checkpoint.pth\nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n                                                                           \nEpoch 96/100 completed - Avg Loss: -0.9338\n                                                                           \nEpoch 97/100 completed - Avg Loss: -0.9341\n                                                                           \nEpoch 98/100 completed - Avg Loss: -0.9315\n                                                                           \nEpoch 99/100 completed - Avg Loss: -0.9322\n                                                                            \nEpoch 100/100 completed - Avg Loss: -0.9313\nSaved full checkpoint for epoch: 100 ->  /kaggle/working/simsiam_task4/ablation_epochs_100/epoch_100/checkpoint.pth       \nSaved ZIP backup to: /kaggle/working/simsiam_ablation_epochs_100.zip\n\n\nAll ablation configs processed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## CELL 2: Ablation Evaluation (epoch_020 -> epoch_100)","metadata":{}},{"cell_type":"code","source":"import os, json, time, shutil, glob\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, silhouette_score\nfrom sklearn.preprocessing import label_binarize\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nBACKBONE = \"resnet18\"\nRESOLUTION = 224\nBATCH_SIZE = 64\nNUM_WORKERS = 2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nOUT_ROOT = \"/kaggle/working/task7_ablation\"\nos.makedirs(OUT_ROOT, exist_ok=True)\n\n\nSPLIT_MANIFEST = \"/kaggle/working/corrected_split_manifest.json\"   \nABLATION_BASE = \"/kaggle/output/all-files/All Files/simsiam_ablation_epochs_100\"  \n\n\nCLASSIFIERS = {\n    \"LogisticRegression\": LogisticRegression(max_iter=2000),\n    \"SVM_RBF\": SVC(kernel=\"rbf\", probability=True),\n    \"RandomForest\": RandomForestClassifier(n_estimators=100),\n    \"DecisionTree\": DecisionTreeClassifier(),\n    \"MLP\": MLPClassifier(hidden_layer_sizes=(512,), max_iter=500)\n}\n\n\nDO_FINETUNE = False   \nRUN_LABEL_EFFICIENCY = True  \n\n\ndef build_encoder(backbone=\"resnet18\"):\n    if backbone == \"resnet18\":\n        base = models.resnet18(weights=None)\n        feat_dim = 512\n    elif backbone == \"resnet50\":\n        base = models.resnet50(weights=None)\n        feat_dim = 2048\n    else:\n        raise ValueError(\"Unsupported backbone\")\n    modules = list(base.children())[:-1]\n    encoder = nn.Sequential(*modules)\n    encoder.feat_dim = feat_dim\n    return encoder\n\ndef try_load_encoder(encoder, path):\n    ck = torch.load(path, map_location=\"cpu\")\n    \n    if isinstance(ck, dict):\n        \n        for key in [\"encoder_state_dict\", \"encoder\", \"model_state\", \"model\", \"state_dict\"]:\n            if key in ck:\n                st = ck[key]\n                break\n        else:\n            st = ck\n    else:\n        st = ck\n        \n    try:\n        encoder.load_state_dict(st)\n        return True\n    except Exception:\n        mapped = {}\n        for k,v in st.items():\n            newk = k\n            if k.startswith(\"encoder.\"):\n                newk = k.replace(\"encoder.\", \"\")\n            if k.startswith(\"module.encoder.\"):\n                newk = k.replace(\"module.encoder.\", \"\")\n            mapped[newk] = v\n        try:\n            encoder.load_state_dict(mapped)\n            return True\n        except Exception as e:\n            print(\"Failed to load encoder weights from\", path, \"error:\", e)\n            return False\n\n\nclass ManifestDataset(Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        lbl = self.labels[idx]\n        img = Image.open(p).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, lbl, p\n\neval_transform = transforms.Compose([\n    transforms.Resize(int(RESOLUTION * 1.1)),\n    transforms.CenterCrop(RESOLUTION),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\ndef extract_features(encoder, paths, batch_size=64, workers=2, save_path=None):\n    ds = ManifestDataset(paths, [0]*len(paths), transform=eval_transform)\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=workers)\n    feats = []\n    encoder.eval()\n    enc = encoder.to(DEVICE)\n    with torch.no_grad():\n        for imgs, _, _ in loader:\n            imgs = imgs.to(DEVICE)\n            h = enc(imgs).view(imgs.size(0), -1).cpu().numpy()\n            feats.append(h)\n    feats = np.vstack(feats)\n    if save_path:\n        np.save(save_path, feats)\n    return feats\n\ndef train_and_eval_probes(train_feats, train_labels, val_feats, val_labels, test_feats, test_labels, out_prefix):\n    results = {}\n    for name, clf in CLASSIFIERS.items():\n        print(\"Training probe:\", name)\n        \n        import copy\n        clf_local = copy.deepcopy(clf)\n        clf_local.fit(train_feats, train_labels)\n        \n        joblib.dump(clf_local, out_prefix + f\"_{name}.joblib\")\n        \n        y_pred = clf_local.predict(test_feats)\n        acc = accuracy_score(test_labels, y_pred)\n        prec, rec, f1, supp = precision_recall_fscore_support(test_labels, y_pred, average=None, zero_division=0)\n        \n        aucs = {}\n        try:\n            if hasattr(clf_local, \"predict_proba\"):\n                probs = clf_local.predict_proba(test_feats)\n                y_bin = label_binarize(test_labels, classes=list(range(np.max(test_labels)+1)))\n                \n                for i in range(y_bin.shape[1]):\n                    try:\n                        aucs[i] = float(roc_auc_score(y_bin[:,i], probs[:,i]))\n                    except Exception:\n                        aucs[i] = None\n                macro = roc_auc_score(y_bin, probs, average=\"macro\")\n                micro = roc_auc_score(y_bin, probs, average=\"micro\")\n            else:\n                aucs = None\n                macro = None; micro = None\n        except Exception as e:\n            aucs = None; macro = None; micro = None\n        results[name] = {\n            \"accuracy\": float(acc),\n            \"per_class_prec\": prec.tolist(),\n            \"per_class_rec\": rec.tolist(),\n            \"per_class_f1\": f1.tolist(),\n            \"per_class_support\": supp.tolist(),\n            \"per_class_auc\": aucs,\n            \"macro_auc\": float(macro) if macro is not None else None,\n            \"micro_auc\": float(micro) if micro is not None else None\n        }\n    return results\n\n\nwith open(SPLIT_MANIFEST, \"r\") as f:\n    split = json.load(f)\ntrain_paths = split[\"train\"]\ntrain_labels = split[\"train_labels\"]\nval_paths = split[\"val\"]\nval_labels = split[\"val_labels\"]\ntest_paths = split[\"test\"]\ntest_labels = split[\"test_labels\"]\nclasses = split.get(\"classes\", None)\nif classes is None:\n    classes = [str(i) for i in range(max(train_labels)+1)]\n\n\nepoch_dirs = sorted(glob.glob(os.path.join(ABLATION_BASE, \"epoch_*\")))\nprint(\"Found ablation epoch dirs:\", epoch_dirs)\n\nablation_summary = []\nfor ed in epoch_dirs:\n    try:\n        epoch_name = os.path.basename(ed)\n        enc_path = os.path.join(ed, \"encoder.pth\")\n        if not os.path.exists(enc_path):\n            print(\"No encoder.pth in\", ed, \"skipping\")\n            continue\n        print(\"Processing\", epoch_name)\n        out_dir = os.path.join(OUT_ROOT, epoch_name)\n        os.makedirs(out_dir, exist_ok=True)\n        \n        encoder = build_encoder(BACKBONE)\n        ok = try_load_encoder(encoder, enc_path)\n        if not ok:\n            print(\"Failed to load encoder for\", epoch_name)\n            continue\n\n        \n        train_feat_path = os.path.join(out_dir, \"train_feats.npy\")\n        val_feat_path = os.path.join(out_dir, \"val_feats.npy\")\n        test_feat_path = os.path.join(out_dir, \"test_feats.npy\")\n\n        if not (os.path.exists(train_feat_path) and os.path.exists(val_feat_path) and os.path.exists(test_feat_path)):\n            print(\"Extracting features for\", epoch_name)\n            tr_feats = extract_features(encoder, train_paths, batch_size=BATCH_SIZE, workers=NUM_WORKERS, save_path=train_feat_path)\n            v_feats = extract_features(encoder, val_paths, batch_size=BATCH_SIZE, workers=NUM_WORKERS, save_path=val_feat_path)\n            te_feats = extract_features(encoder, test_paths, batch_size=BATCH_SIZE, workers=NUM_WORKERS, save_path=test_feat_path)\n        else:\n            print(\"Loading cached features for\", epoch_name)\n            tr_feats = np.load(train_feat_path)\n            v_feats = np.load(val_feat_path)\n            te_feats = np.load(test_feat_path)\n\n        \n        try:\n            feats_all = np.vstack([tr_feats, v_feats, te_feats])\n            lbls_all = np.array(train_labels + val_labels + test_labels)\n            sil = silhouette_score(feats_all, lbls_all) if len(np.unique(lbls_all))>1 else None\n        except Exception as e:\n            print(\"Silhouette failed:\", e)\n            sil = None\n\n        \n        probe_results = train_and_eval_probes(tr_feats, train_labels, v_feats, val_labels, te_feats, test_labels, out_prefix=os.path.join(out_dir, \"probe\"))\n\n        \n        label_eff = {}\n        if RUN_LABEL_EFFICIENCY:\n            fractions = [0.01, 0.05, 0.10, 0.25, 0.50, 1.0]\n            total = tr_feats.shape[0]\n            for frac in fractions:\n                n = max(1, int(total * frac))\n                \n                idxs = np.arange(total)\n                rng = np.random.RandomState(42)\n                \n                sel = rng.choice(idxs, size=n, replace=False)\n                clf = LogisticRegression(max_iter=2000)\n                clf.fit(tr_feats[sel], np.array(train_labels)[sel])\n                pred = clf.predict(te_feats)\n                acc = accuracy_score(test_labels, pred)\n                label_eff[f\"{int(frac*100)}%\"] = float(acc)\n\n            # save\n            with open(os.path.join(out_dir, \"label_efficiency.json\"), \"w\") as f:\n                json.dump(label_eff, f, indent=2)\n\n        \n        summary = {\n            \"epoch_dir\": epoch_name,\n            \"enc_path\": enc_path,\n            \"silhouette\": float(sil) if sil is not None else None,\n            \"probe_results\": probe_results,\n            \"label_efficiency\": label_eff\n        }\n        ablation_summary.append(summary)\n        with open(os.path.join(out_dir, \"ablation_summary.json\"), \"w\") as f:\n            json.dump(summary, f, indent=2)\n\n        \n        for name in probe_results.keys():\n            \n            try:\n                clf = joblib.load(os.path.join(out_dir, f\"probe_{name}.joblib\"))\n            except Exception:\n                clf = None\n            if clf is None: continue\n            y_pred = clf.predict(te_feats)\n            cm = confusion_matrix(test_labels, y_pred)\n            plt.figure(figsize=(6,5))\n            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n            plt.title(f\"Confusion Matrix - {epoch_name} - {name}\")\n            plt.savefig(os.path.join(out_dir, f\"confusion_{name}.png\"))\n            plt.close()\n\n    except Exception as e:\n        print(\"Error processing\", ed, e)\n\n\nwith open(os.path.join(OUT_ROOT, \"ablation_results.json\"), \"w\") as f:\n    json.dump(ablation_summary, f, indent=2)\n\n\nimport csv\ncsv_path = os.path.join(OUT_ROOT, \"ablation_table.csv\")\nwith open(csv_path, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"epoch_dir\",\"silhouette\",\"probe_name\",\"accuracy\",\"macro_auc\",\"micro_auc\"])\n    for s in ablation_summary:\n        ed = s[\"epoch_dir\"]\n        sil = s[\"silhouette\"]\n        for probe_name, pr in s[\"probe_results\"].items():\n            writer.writerow([ed, sil, probe_name, pr.get(\"accuracy\", None), pr.get(\"macro_auc\", None), pr.get(\"micro_auc\", None)])\n\n\nzipname = os.path.join(\"/kaggle/working\", \"task7_ablation_outputs\")\nif os.path.exists(zipname + \".zip\"):\n    os.remove(zipname + \".zip\")\nshutil.make_archive(base_name=zipname, format=\"zip\", root_dir=OUT_ROOT)\nprint(\"Ablation outputs zipped to\", zipname + \".zip\")\nprint(\"Done. Outputs in\", OUT_ROOT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:31:55.185752Z","iopub.execute_input":"2025-12-06T12:31:55.186079Z","iopub.status.idle":"2025-12-06T13:37:06.855478Z","shell.execute_reply.started":"2025-12-06T12:31:55.186057Z","shell.execute_reply":"2025-12-06T13:37:06.854667Z"}},"outputs":[{"name":"stdout","text":"Found ablation epoch dirs: ['/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_020', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_040', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_060', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_080', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_085', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_090', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_095', '/kaggle/input/all-files/All Files/simsiam_ablation_epochs_100/epoch_100']\nProcessing epoch_020\nExtracting features for epoch_020\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_040\nExtracting features for epoch_040\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_060\nExtracting features for epoch_060\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_080\nExtracting features for epoch_080\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_085\nExtracting features for epoch_085\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_090\nExtracting features for epoch_090\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_095\nExtracting features for epoch_095\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nProcessing epoch_100\nExtracting features for epoch_100\nTraining probe: LogisticRegression\nTraining probe: SVM_RBF\nTraining probe: RandomForest\nTraining probe: DecisionTree\nTraining probe: MLP\nAblation outputs zipped to /kaggle/working/task7_ablation_outputs.zip\nDone. Outputs in /kaggle/working/task7_ablation\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Cell 3: Ratio Sweep Evaluation","metadata":{}},{"cell_type":"code","source":"import os, json, time, shutil\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.preprocessing import label_binarize\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.random as npr\n\n\nBACKBONE = \"resnet18\"\nRESOLUTION = 224\nBATCH_SIZE = 64\nNUM_WORKERS = 2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nOUT_ROOT = \"/kaggle/working/task7_ratios\"\nos.makedirs(OUT_ROOT, exist_ok=True)\n\n\nSPLIT_MANIFEST = \"/kaggle/input/simsiam-task4-archive/split_manifest.json\"\nBASE_FEATURES_DIR = \"/kaggle/input/all-files/All Files\"\nBASE_TRAIN_FEATS = os.path.join(BASE_FEATURES_DIR, \"train_feats.npy\")\nBASE_TRAIN_LABELS = os.path.join(BASE_FEATURES_DIR, \"train_labels.npy\")\nBASE_VAL_FEATS = os.path.join(BASE_FEATURES_DIR, \"val_feats.npy\")\nBASE_VAL_LABELS = os.path.join(BASE_FEATURES_DIR, \"val_labels.npy\")\nBASE_TEST_FEATS = os.path.join(BASE_FEATURES_DIR, \"test_feats.npy\")\nBASE_TEST_LABELS = os.path.join(BASE_FEATURES_DIR, \"test_labels.npy\")\n\nCLASSIFIERS = {\n    \"LogisticRegression\": LogisticRegression(max_iter=2000),\n    \"SVM_RBF\": SVC(kernel=\"rbf\", probability=True),\n    \"RandomForest\": RandomForestClassifier(n_estimators=100),\n    \"DecisionTree\": DecisionTreeClassifier(),\n    \"MLP\": MLPClassifier(hidden_layer_sizes=(512,), max_iter=500)\n}\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        lbl = self.labels[idx]\n        img = Image.open(p).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, lbl, p\n\neval_transform = transforms.Compose([\n    transforms.Resize(int(RESOLUTION * 1.1)),\n    transforms.CenterCrop(RESOLUTION),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\ndef build_encoder(backbone=\"resnet18\"):\n    if backbone == \"resnet18\":\n        base = models.resnet18(weights=None)\n        feat_dim = 512\n    elif backbone == \"resnet50\":\n        base = models.resnet50(weights=None)\n        feat_dim = 2048\n    else:\n        raise ValueError(\"Unsupported backbone\")\n    modules = list(base.children())[:-1]\n    encoder = nn.Sequential(*modules)\n    encoder.feat_dim = feat_dim\n    return encoder\n\ndef load_encoder_weights(encoder, ckpt_path):\n    ck = torch.load(ckpt_path, map_location=\"cpu\")\n    if isinstance(ck, dict):\n        for key in [\"encoder_state_dict\",\"encoder\",\"model_state\",\"state_dict\",\"model\"]:\n            if key in ck:\n                state = ck[key]; break\n        else:\n            state = ck\n    else:\n        state = ck\n    try:\n        encoder.load_state_dict(state)\n        return True\n    except Exception:\n        mapped = {}\n        for k,v in state.items():\n            newk = k\n            if k.startswith(\"encoder.\"): newk = k.replace(\"encoder.\",\"\")\n            if k.startswith(\"module.encoder.\"): newk = k.replace(\"module.encoder.\",\"\")\n            mapped[newk] = v\n        try:\n            encoder.load_state_dict(mapped)\n            return True\n        except Exception as e:\n            print(\"Failed to load encoder:\", e)\n            return False\n\ndef extract_features_from_paths(encoder, paths, batch_size=64, workers=2, save_path=None):\n    ds = ImageDataset(paths, [0]*len(paths), transform=eval_transform)\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=workers)\n    feats = []\n    encoder.eval()\n    enc = encoder.to(DEVICE)\n    with torch.no_grad():\n        for imgs, _, _ in loader:\n            imgs = imgs.to(DEVICE)\n            h = enc(imgs).view(imgs.size(0), -1).cpu().numpy()\n            feats.append(h)\n    feats = np.vstack(feats)\n    if save_path: np.save(save_path, feats)\n    return feats\n\n\ndef stratified_subsample_indices(y, k, random_state=42):\n    y = np.array(y)\n    n = len(y)\n    unique, counts = np.unique(y, return_counts=True)\n    n_classes = len(unique)\n    if n_classes < 2:\n        return None\n    if k < 2:\n        return None\n    rng = npr.RandomState(random_state)\n    try:\n        \n        train_idx, _ = train_test_split(np.arange(n), train_size=k, stratify=y, random_state=random_state)\n        if len(np.unique(y[train_idx])) >= 2:\n            return train_idx\n    except Exception:\n        pass\n        \n    prop = counts / counts.sum()\n    desired = np.floor(prop * k).astype(int)\n    for i, uc in enumerate(unique):\n        if desired[i] == 0 and k >= n_classes and counts[i] > 0:\n            desired[i] = 1\n    rem = int(k - desired.sum())\n    if rem > 0:\n        leftover = (prop * k) - desired\n        order = np.argsort(-leftover)\n        for idx in order:\n            if rem <= 0:\n                break\n            desired[idx] += 1\n            rem -= 1\n    selected = []\n    for cls_idx, cls in enumerate(unique):\n        cls_inds = np.where(y == cls)[0]\n        cnt = desired[cls_idx]\n        if cnt <= 0:\n            continue\n        if cnt > len(cls_inds):\n            cnt = len(cls_inds)\n        chosen = rng.choice(cls_inds, size=cnt, replace=False)\n        selected.extend(chosen.tolist())\n    selected = np.array(selected, dtype=int)\n    if len(selected) < 2 or len(np.unique(y[selected])) < 2:\n        return None\n    if len(selected) > k:\n        selected = selected[:k]\n    return selected\n\n\nwith open(SPLIT_MANIFEST, \"r\") as f:\n    sm = json.load(f)\n\n\nfeats_all = None\nlabels_all = None\n\nhave_train = os.path.exists(BASE_TRAIN_FEATS) and os.path.exists(BASE_TRAIN_LABELS)\nhave_val = os.path.exists(BASE_VAL_FEATS) and os.path.exists(BASE_VAL_LABELS)\nhave_test = os.path.exists(BASE_TEST_FEATS) and os.path.exists(BASE_TEST_LABELS)\n\nif have_train and have_val and have_test:\n    tr = np.load(BASE_TRAIN_FEATS); tr_lbl = np.load(BASE_TRAIN_LABELS)\n    v = np.load(BASE_VAL_FEATS); v_lbl = np.load(BASE_VAL_LABELS)\n    te = np.load(BASE_TEST_FEATS); te_lbl = np.load(BASE_TEST_LABELS)\n    feats_all = np.vstack([tr, v, te])\n    labels_all = np.hstack([tr_lbl, v_lbl, te_lbl]).astype(int)\nelif have_train and have_test:\n    tr = np.load(BASE_TRAIN_FEATS); tr_lbl = np.load(BASE_TRAIN_LABELS)\n    te = np.load(BASE_TEST_FEATS); te_lbl = np.load(BASE_TEST_LABELS)\n    feats_all = np.vstack([tr, te])\n    labels_all = np.hstack([tr_lbl, te_lbl]).astype(int)\nelse:\n    \n    print(\"No adequate cached features found, will extract features from images.\")\n    all_paths = sm[\"train\"] + sm[\"val\"] + sm[\"test\"]\n    all_labels = sm[\"train_labels\"] + sm[\"val_labels\"] + sm[\"test_labels\"]\n    BASE_ENCODER_CKPT = \"/kaggle/input/simsiam-task4-archive/simsiam_encoder.pth\"\n    encoder = build_encoder(BACKBONE)\n    if not load_encoder_weights(encoder, BASE_ENCODER_CKPT):\n        raise RuntimeError(\"Cannot load encoder weights from \" + BASE_ENCODER_CKPT)\n    feats_all = extract_features_from_paths(encoder, all_paths, batch_size=BATCH_SIZE, workers=NUM_WORKERS, save_path=os.path.join(OUT_ROOT,\"feats_all.npy\"))\n    labels_all = np.array(all_labels).astype(int)\n    np.save(os.path.join(OUT_ROOT,\"labels_all.npy\"), labels_all)\n\n\nif feats_all is None or labels_all is None:\n    raise RuntimeError(\"Failed to prepare features and labels.\")\n\nN = feats_all.shape[0]\nprint(\"Total samples used for ratio sweep:\", N)\n\n\nnp.save(os.path.join(OUT_ROOT,\"feats_all.npy\"), feats_all)\nnp.save(os.path.join(OUT_ROOT,\"labels_all.npy\"), labels_all)\n\n\nratios = [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\nratio_results = []\n\nfor train_frac in ratios:\n    \n    n_train = int(round(train_frac * N))\n    \n    if n_train >= N:\n        n_train = N - 1\n    if n_train < 1:\n        n_train = 1\n    n_test = N - n_train\n    print(f\"Running ratio: train {int(train_frac*100)}% -> n_train={n_train}, n_test={n_test}\")\n    \n    idxs = np.arange(N)\n    try:\n        train_idx, test_idx = train_test_split(idxs, train_size=n_train, stratify=labels_all, random_state=42)\n    except Exception as e:\n        \n        print(\"Stratified split failed, falling back to random split:\", e)\n        train_idx, test_idx = train_test_split(idxs, train_size=n_train, random_state=42)\n    X_train = feats_all[train_idx]; y_train = labels_all[train_idx]\n    X_test = feats_all[test_idx]; y_test = labels_all[test_idx]\n    \n    val_portion = max(1, int(round(0.10 * len(X_train))))\n    \n    try:\n        tr_sub_idx, val_sub_idx = train_test_split(np.arange(len(X_train)), test_size=val_portion, stratify=y_train, random_state=42)\n    except Exception:\n        tr_sub_idx, val_sub_idx = train_test_split(np.arange(len(X_train)), test_size=val_portion, random_state=42)\n    X_tr = X_train[tr_sub_idx]; y_tr = y_train[tr_sub_idx]\n    X_val = X_train[val_sub_idx]; y_val = y_train[val_sub_idx]\n\n    \n    probes_res = {}\n    for name, clf in CLASSIFIERS.items():\n        import copy\n        clf_local = copy.deepcopy(clf)\n        if len(np.unique(y_tr)) < 2:\n            probes_res[name] = {\"accuracy\": None, \"macro_auc\": None, \"micro_auc\": None, \"per_class_f1\": None, \"skipped_reason\": \"only_one_class_in_train\"}\n            continue\n        clf_local.fit(X_tr, y_tr)\n        joblib.dump(clf_local, os.path.join(OUT_ROOT, f\"{int(train_frac*100)}pct_{name}.joblib\"))\n        y_pred = clf_local.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        prec, rec, f1, sup = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n        \n        try:\n            if hasattr(clf_local, \"predict_proba\"):\n                probs = clf_local.predict_proba(X_test)\n                y_bin = label_binarize(y_test, classes=list(range(np.max(labels_all)+1)))\n                macro = roc_auc_score(y_bin, probs, average=\"macro\")\n                micro = roc_auc_score(y_bin, probs, average=\"micro\")\n            else:\n                macro = None; micro = None\n        except Exception:\n            macro = None; micro = None\n        probes_res[name] = {\"accuracy\": float(acc), \"macro_auc\": macro, \"micro_auc\": micro, \"per_class_f1\": f1.tolist()}\n\n    \n    label_eff = {}\n    total = X_tr.shape[0]\n    for frac in [0.01,0.05,0.10,0.25,0.50,1.0]:\n        k = max(1, int(total * frac))\n        sel = stratified_subsample_indices(y_tr, k, random_state=42)\n        if sel is None:\n            label_eff[f\"{int(frac*100)}%\"] = {\"accuracy\": None, \"skipped\": True, \"reason\": \"insufficient_class_diversity_for_k\"}\n            continue\n        clf = LogisticRegression(max_iter=2000)\n        clf.fit(X_tr[sel], y_tr[sel])\n        score = accuracy_score(y_test, clf.predict(X_test))\n        label_eff[f\"{int(frac*100)}%\"] = {\"accuracy\": float(score), \"skipped\": False, \"k_used\": int(len(sel))}\n\n    ratio_results.append({\n        \"train_frac\": train_frac,\n        \"n_train\": int(n_train),\n        \"n_val\": int(len(X_val)),\n        \"n_test\": int(n_test),\n        \"probes\": probes_res,\n        \"label_efficiency\": label_eff\n    })\n\n    \n    with open(os.path.join(OUT_ROOT, f\"ratio_{int(train_frac*100)}.json\"), \"w\") as f:\n        json.dump(ratio_results[-1], f, indent=2)\n\n        \nwith open(os.path.join(OUT_ROOT, \"ratio_results.json\"), \"w\") as f:\n    json.dump(ratio_results, f, indent=2)\n\nimport csv\ncsv_path = os.path.join(OUT_ROOT, \"ratio_table.csv\")\nwith open(csv_path, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    \n    header = [\"train_pct\", \"n_train\", \"n_val\", \"n_test\"]\n    probe_names = list(CLASSIFIERS.keys())\n    header += [pn + \"_acc\" for pn in probe_names]\n    writer.writerow(header)\n    for rr in ratio_results:\n        row = [int(rr[\"train_frac\"]*100), rr[\"n_train\"], rr[\"n_val\"], rr[\"n_test\"]]\n        for pn in probe_names:\n            acc_val = rr[\"probes\"][pn][\"accuracy\"] if rr[\"probes\"][pn].get(\"accuracy\") is not None else \"\"\n            row.append(acc_val)\n        writer.writerow(row)\n\n\nzipname = os.path.join(\"/kaggle/working\", \"task7_ratio_outputs\")\nif os.path.exists(zipname + \".zip\"):\n    os.remove(zipname + \".zip\")\nshutil.make_archive(base_name=zipname, format=\"zip\", root_dir=OUT_ROOT)\nprint(\"Ratio sweep outputs zipped to\", zipname + \".zip\")\nprint(\"Done. Outputs in\", OUT_ROOT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:00:38.293061Z","iopub.execute_input":"2025-12-06T15:00:38.293745Z","iopub.status.idle":"2025-12-06T15:03:46.259406Z","shell.execute_reply.started":"2025-12-06T15:00:38.293719Z","shell.execute_reply":"2025-12-06T15:03:46.258575Z"}},"outputs":[{"name":"stdout","text":"Total samples used for ratio sweep: 1800\nRunning ratio: train 90% -> n_train=1620, n_test=180\nRunning ratio: train 80% -> n_train=1440, n_test=360\nRunning ratio: train 70% -> n_train=1260, n_test=540\nRunning ratio: train 60% -> n_train=1080, n_test=720\nRunning ratio: train 50% -> n_train=900, n_test=900\nRunning ratio: train 40% -> n_train=720, n_test=1080\nRunning ratio: train 30% -> n_train=540, n_test=1260\nRunning ratio: train 20% -> n_train=360, n_test=1440\nRunning ratio: train 10% -> n_train=180, n_test=1620\nRatio sweep outputs zipped to /kaggle/working/task7_ratio_outputs.zip\nDone. Outputs in /kaggle/working/task7_ratios\n","output_type":"stream"}],"execution_count":9}]}